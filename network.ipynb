{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "ZuOUTHGyR1OZ"
   },
   "source": [
    "# Практическое задание 2\n",
    "\n",
    "\n",
    "\n",
    "## Замечания\n",
    "\n",
    "* Задание необходимо сдать боту до 06.12.2021\n",
    "* Соблюдаем кодекс чести (по нулям и списавшему, и давшему списать)\n",
    "* Можно (и нужно!) применять для реализации только библиотеку **Numpy**\n",
    "* Ничего, крому Numpy, нельзя использовать для реализации \n",
    "* **Keras** используется только для тестирования Вашей реализации\n",
    "* Если какой-то из классов не проходит приведенные тесты, то соответствующее задание не оценивается\n",
    "* Возможно использование дополнительных (приватных) тестов\n",
    " "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "R4H9YMxvR1Oe"
   },
   "source": [
    "## Реализация собственного нейросетевого пакета для запуска и обучения нейронных сетей"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "q3wsg4K_R1Oe"
   },
   "source": [
    "Задание состоит из трёх частей:\n",
    "1. Реализация прямого вывода нейронной сети (первое практическое задание)\n",
    "2. Реализация градиентов по входу и распространения градиента по сети (back propagation)\n",
    "3. Реализация градиентов по параметрам и метода обратного распространения ошибки с обновлением парметров сети"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "30eO8eDJR1Of"
   },
   "source": [
    "###  1. Реализация вывода собственной нейронной сети"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "tFUJYwskR1Of"
   },
   "source": [
    "1.1 Внимательно ознакомьтесь с интерфейсом слоя. Любой слой должен содержать как минимум три метода:\n",
    "- конструктор\n",
    "- прямой вывод \n",
    "- обратный вывод, производные по входу и по параметрам"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "id": "oGDp57wxR1Og"
   },
   "outputs": [],
   "source": [
    "class Layer(object):\n",
    "    def __init__(self):\n",
    "        self.name = 'Layer'       \n",
    "    def forward(self, input_data):\n",
    "        pass\n",
    "    def backward(self, input_data):\n",
    "        return [self.grad_x(input_data), self.grad_param(input_data)]\n",
    "    \n",
    "    def grad_x(self, input_data):\n",
    "        pass\n",
    "    def grad_param(self, input_data):\n",
    "        return []\n",
    "    \n",
    "    def update_param(self, grads, learning_rate):\n",
    "        pass\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "pvgq5a9NR1Oh"
   },
   "source": [
    "1.2 Ниже предствален интерфейс класса  Network. Обратите внимание на реализацию метода predict, который последовательно обрабатывает входные данные слой за слоем."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "id": "mBtrhe1vR1Oi"
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from sklearn.model_selection import train_test_split\n",
    "from tqdm import tqdm\n",
    "\n",
    "class Network(object):\n",
    "    def __init__(self, layers, loss=None):\n",
    "        self.name = 'Network'\n",
    "        self.layers = layers\n",
    "        self.loss = loss\n",
    "    \n",
    "    def forward(self, input_data):\n",
    "        return self.predict(input_data)\n",
    "\n",
    "    def grad_x(self, input_data, labels):\n",
    "        gradients_x = []\n",
    "        current_input = input_data\n",
    "        for layer in self.layers:\n",
    "            gradients_x.append(layer.grad_x(current_input))\n",
    "            current_input = layer.forward(current_input) \n",
    "        gradient_x = self.loss.grad_x(current_input, labels)\n",
    "        gradient_x = np.reshape(gradient_x, (gradient_x.shape[0], 1, gradient_x.shape[1]))\n",
    "        for gradient_x_layer in reversed(gradients_x):\n",
    "            gradient_x = np.matmul(gradient_x, gradient_x_layer)\n",
    "        gradient_x = np.reshape(gradient_x, (gradient_x.shape[0], gradient_x.shape[2]))\n",
    "        return gradient_x\n",
    "\n",
    "    def grad_param(self, input_data, labels):\n",
    "        gradients_param = []\n",
    "        current_input = input_data\n",
    "        for layer in self.layers:\n",
    "            gradients_param.append(layer.backward(current_input))\n",
    "            current_input = layer.forward(current_input) \n",
    "        gradients_param.append([self.loss.grad_x(current_input, labels), []])\n",
    "        return gradients_param\n",
    "\n",
    "    def update(self, grad_list, learning_rate):\n",
    "        gradient_x = grad_list[-1][0]\n",
    "        gradient_x = np.reshape(gradient_x, (gradient_x.shape[0], 1, gradient_x.shape[1]))\n",
    "        for i in range(len(grad_list)-2, -1, -1):\n",
    "            layer = self.layers[i]\n",
    "            if len(grad_list[i][1]) != 0:\n",
    "                layer.update_param([np.matmul(gradient_x, grad_list[i][1][0]), np.matmul(gradient_x, grad_list[i][1][1])], learning_rate)\n",
    "            gradient_x = np.matmul(gradient_x, grad_list[i][0])\n",
    "    \n",
    "    def predict(self, input_data):\n",
    "        current_input = input_data\n",
    "        for layer in self.layers:\n",
    "            current_input = layer.forward(current_input)     \n",
    "        return current_input\n",
    "    \n",
    "    def calculate_loss(self, input_data, labels):\n",
    "        return self.loss.forward(self.predict(input_data), labels)\n",
    "    \n",
    "    def train_step(self, input_data, labels, learning_rate=0.001):\n",
    "        grad_list = self.grad_param(input_data, labels)\n",
    "        self.update(grad_list, learning_rate)\n",
    "    \n",
    "    def fit(self, trainX, trainY, validation_split=0.25, \n",
    "            batch_size=1, nb_epoch=1, learning_rate=0.01):\n",
    "        \n",
    "        train_x, val_x, train_y, val_y = train_test_split(trainX, trainY, \n",
    "                                                          test_size=validation_split,\n",
    "                                                          random_state=42)\n",
    "        for epoch in range(nb_epoch):\n",
    "            #train one epoch\n",
    "            for i in tqdm(range(int(len(train_x)/batch_size))):\n",
    "                batch_x = train_x[i*batch_size: (i+1)*batch_size]\n",
    "                batch_y = train_y[i*batch_size: (i+1)*batch_size]\n",
    "                self.train_step(batch_x, batch_y, learning_rate)\n",
    "            #validate\n",
    "            val_accuracy = self.evaluate(val_x, val_y)\n",
    "            print(\"\\n\",'%d epoch: val %.2f' %(epoch+1, val_accuracy), \"\\n\")\n",
    "            \n",
    "    def evaluate(self, testX, testY):\n",
    "        y_pred = np.argmax(self.predict(testX), axis=1)            \n",
    "        y_true = np.argmax(testY, axis=1)\n",
    "        val_accuracy = np.sum((y_pred == y_true))/(len(y_true))\n",
    "        return val_accuracy"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "xGGs9_tkR1Oj"
   },
   "source": [
    "#### 1.1 (6 баллов) Необходимо реализовать метод forward для вычисления следующих слоёв:\n",
    "\n",
    "- DenseLayer\n",
    "- ReLU\n",
    "- Softmax\n",
    "- FlattenLayer\n",
    "- MaxPooling"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "id": "98A0fEXyR1Ok"
   },
   "outputs": [],
   "source": [
    "#импорты\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "id": "4tojxogmR1Ol"
   },
   "outputs": [],
   "source": [
    "class DenseLayer(Layer):\n",
    "    def __init__(self, input_dim, output_dim, W_init=None, b_init=None):\n",
    "        self.name = 'Dense'\n",
    "        self.input_dim = input_dim\n",
    "        self.output_dim = output_dim\n",
    "        if W_init is None or b_init is None:\n",
    "            self.W = np.random.normal(0, np.sqrt(2. / input_dim), size = (input_dim, output_dim))\n",
    "            self.b = np.zeros(output_dim, 'float32')\n",
    "        else:\n",
    "            self.W = W_init\n",
    "            self.b = b_init\n",
    "    def forward(self, input_data):\n",
    "        assert input_data.shape[1] == self.W.shape[0]\n",
    "        out = np.matmul(input_data, self.W) + self.b\n",
    "        return out\n",
    "    def grad_x(self, input_data):\n",
    "        (input_dim, output_dim) = (self.input_dim, self.output_dim)\n",
    "        out = np.zeros(shape = (input_data.shape[0], output_dim, input_dim))\n",
    "        for batch_id in range(input_data.shape[0]):\n",
    "            array = input_data[batch_id]\n",
    "            for out_id in range(output_dim):\n",
    "                for in_id in range(input_dim):\n",
    "                    out[batch_id, out_id, in_id] = self.W[in_id, out_id]\n",
    "        return out\n",
    "    def grad_b(self, input_data):\n",
    "        (input_dim, output_dim) = (self.input_dim, self.output_dim)\n",
    "        out = np.zeros(shape = (input_data.shape[0], output_dim, output_dim))\n",
    "        for batch_id in range(input_data.shape[0]):\n",
    "            for out_id in range(output_dim):\n",
    "                out[batch_id, out_id, out_id] = 1\n",
    "        return out\n",
    "    def grad_W(self, input_data):\n",
    "        (input_dim, output_dim) = (self.input_dim, self.output_dim)\n",
    "        out = np.zeros(shape = (input_data.shape[0], output_dim, input_dim * output_dim))\n",
    "        for batch_id in range(input_data.shape[0]):\n",
    "            for k in range(output_dim):\n",
    "                # d out_j / d w_s_t, if j != t zero\n",
    "                for j in range(input_dim):\n",
    "                    out[batch_id, k, j*output_dim + k] = input_data[batch_id, j]\n",
    "        return out\n",
    "    \n",
    "    def update_W(self, grad, learning_rate):\n",
    "        self.W -= learning_rate * np.mean(grad, axis=0).reshape(self.W.shape)\n",
    "    \n",
    "    def update_b(self, grad,  learning_rate):\n",
    "        self.b -= learning_rate * np.mean(grad, axis=0).reshape(self.b.shape)\n",
    "        \n",
    "    def update_param(self, params_grad, learning_rate):\n",
    "        self.update_W(params_grad[0], learning_rate)\n",
    "        self.update_b(params_grad[1], learning_rate)\n",
    "    \n",
    "    def grad_param(self, input_data):\n",
    "        return [self.grad_W(input_data), self.grad_b(input_data)]\n",
    "    \n",
    "\n",
    "class ReLU(Layer):\n",
    "    def __init__(self):\n",
    "        self.name = 'ReLU'\n",
    "    def forward(self, input_data):\n",
    "        out = input_data.copy()\n",
    "        out[out < 0] = 0\n",
    "        return out\n",
    "    def grad_x(self, input_data):\n",
    "        if len(input_data.shape) == 2:\n",
    "            (batch_size, logits) = input_data.shape\n",
    "            out = np.zeros(shape = (batch_size, logits, logits))\n",
    "            for batch_id in range(batch_size):\n",
    "                for i in range(logits):\n",
    "                    if input_data[batch_id, i] > 0:\n",
    "                        out[batch_id, i, i] = 1\n",
    "            return out\n",
    "        batch_size = input_data.shape[0]\n",
    "        input_channels = input_data.shape[1]\n",
    "        n = input_data.shape[2]\n",
    "        m = input_data.shape[3]\n",
    "        out = np.zeros(shape = (batch_size, input_channels*n*m, input_channels*n*m))\n",
    "        for batch_id in range(batch_size):\n",
    "            for in_ch in range(input_channels):\n",
    "                for x in range(n):\n",
    "                    for y in range(m):\n",
    "                        if input_data[batch_id, in_ch, x, y] > 0:\n",
    "                            out[batch_id, in_ch*n*m+x*m+y, in_ch*n*m+x*m+y] = 1\n",
    "        return out\n",
    "    \n",
    "    \n",
    "class Softmax(Layer):\n",
    "    def __init__(self):\n",
    "        self.name = 'Softmax'\n",
    "    def forward(self, input_data):\n",
    "        def g(array):\n",
    "            array = np.exp(array)\n",
    "            array /= np.sum(array)\n",
    "            return array\n",
    "        return np.apply_along_axis(g, -1, input_data)\n",
    "    def grad_x(self, input_data):\n",
    "        (batch_size, logits) = input_data.shape\n",
    "        out = np.zeros(shape = (batch_size, logits, logits))\n",
    "        for batch_id in range(batch_size):\n",
    "            array = np.exp(input_data[batch_id])\n",
    "            sum_of_array = np.sum(array)\n",
    "            for i in range(logits):\n",
    "                for j in range(logits):\n",
    "                    out[batch_id, i, j] = - (array[i] * array[j]) / sum_of_array**2\n",
    "                    if i == j:\n",
    "                         out[batch_id, i, j] += array[i] / sum_of_array\n",
    "\n",
    "        return out\n",
    "    \n",
    "\n",
    "class FlattenLayer(Layer):\n",
    "    def __init__(self):\n",
    "        self.name = 'Flatten'\n",
    "    def forward(self, input_data):\n",
    "        length = input_data.shape[1] * input_data.shape[2] \\\n",
    "            * input_data.shape[3]\n",
    "        out = np.zeros((input_data.shape[0], length))\n",
    "        for batch_id in range(input_data.shape[0]):\n",
    "            out[batch_id] = np.transpose(input_data[batch_id], axes = (1, 2, 0)).reshape(length)\n",
    "        return out\n",
    "    def grad_x(self, input_data):\n",
    "        length = input_data.shape[1] * input_data.shape[2] \\\n",
    "            * input_data.shape[3]\n",
    "        out = np.zeros((input_data.shape[0], length, length))\n",
    "        for batch_id in range(input_data.shape[0]):\n",
    "            for i in range(length):\n",
    "                out[batch_id, i, i] = 1\n",
    "        return out\n",
    "\n",
    "\n",
    "class MaxPooling(Layer):\n",
    "    def __init__(self, pool_size=(2, 2), strides=2):\n",
    "        self.name = 'MaxPooling'\n",
    "        self.pool_size = pool_size\n",
    "        self.strides = 2\n",
    "    def forward(self, input_data):\n",
    "        input = input_data.copy()\n",
    "        (k1, k2) = self.pool_size\n",
    "        s = self.strides\n",
    "        (batch_count, channels_count) = (input.shape[0], input.shape[1])\n",
    "        (n, m) = (input.shape[2], input.shape[3])\n",
    "        (out_n, out_m) = ((n - k1 + 1 + s - 1) // s, (m - k2 + 1 + s - 1) // s)\n",
    "        assert out_n >= 0\n",
    "        assert out_m >= 0\n",
    "        out = np.zeros([batch_count, channels_count, out_n, out_m])\n",
    "        for x in range(0, n - k1 + 1, s):\n",
    "            for y in range(0, m - k2 + 1, s):\n",
    "                a = input[:, :, x:x + k1, y:y + k2]\n",
    "                out[:, :, x // s, y // s] = np.amax(a, axis =(2,3))\n",
    "        return out    \n",
    "    def grad_x(self, input_data):\n",
    "        input = input_data.copy()\n",
    "        (k1, k2) = self.pool_size\n",
    "        s = self.strides\n",
    "        (batch_count, channels_count) = (input.shape[0], input.shape[1])\n",
    "        (n, m) = (input.shape[2], input.shape[3])\n",
    "        (out_n, out_m) = ((n - k1 + 1 + s - 1) // s, (m - k2 + 1 + s - 1) // s)\n",
    "        assert out_n >= 0\n",
    "        assert out_m >= 0\n",
    "        out = np.zeros([batch_count, channels_count*out_n*out_m, channels_count*n*m])\n",
    "        for batch_id in range(batch_count):\n",
    "            for out_ch in range(channels_count):\n",
    "                for x in range(0, n - k1 + 1, s):\n",
    "                    for y in range(0, m - k2 + 1, s):\n",
    "                        mymax = np.amax(input[batch_id, out_ch, x:x+k1, y:y+k2])\n",
    "                        for nx in range(x,x+k1):\n",
    "                            for ny in range(y,y+k2):\n",
    "                                if input[batch_id, out_ch, nx, ny] == mymax:\n",
    "                                    id1 = out_ch*out_n*out_m + (x//s)*out_m + (y//s)\n",
    "                                    id2 = out_ch*n*m + nx*m + ny\n",
    "                                    out[batch_id, id1, id2] = 1\n",
    "        return out    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "N1quSgzvR1Ol"
   },
   "source": [
    "#### 1.2 (3 балла) Реализуйте теперь свёртночный слой   (опционально)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "id": "isU5FhifXr3p"
   },
   "outputs": [],
   "source": [
    "class Conv2DLayer(Layer):\n",
    "    def __init__(self, kernel_size=3, input_channels=2, output_channels=3, \n",
    "                 stride=1, K_init=None, b_init=None):\n",
    "      # padding: 'valid'\n",
    "      # Работаем с квадратными ядрами, поэтому kernel_size - одно число\n",
    "      # Работаем с единообразным сдвигом, поэтому stride - одно число\n",
    "      # Фильтр размерности [kernel_size, kernel_size, input_channels, output_channels]\n",
    "      self.name = 'Conv2D'\n",
    "      self.kernel_size = kernel_size\n",
    "      self.input_channels = input_channels\n",
    "      self.output_channels = output_channels\n",
    "      self.stride = stride\n",
    "      if K_init is None or b_init is None:\n",
    "          self.kernel = np.random.normal(0, np.sqrt(2. / (kernel_size*kernel_size*output_channels)), size =  (kernel_size,kernel_size,input_channels,output_channels))\n",
    "          self.bias = np.zeros(output_channels, 'float32')\n",
    "      else:\n",
    "          self.kernel = K_init\n",
    "          self.bias = b_init\n",
    "            \n",
    "    def forward(self, input_data):\n",
    "        def dot(a, b):\n",
    "            a = np.moveaxis(a, 0, -1)\n",
    "            assert a.shape == b.shape\n",
    "            return (a*b).sum()\n",
    "\n",
    "        input = input_data.copy()\n",
    "        assert self.input_channels == input.shape[1]\n",
    "        k = self.kernel_size\n",
    "        s = self.stride\n",
    "        (batch_count, channels_count) = (input.shape[0], input.shape[1])\n",
    "        (n, m) = (input.shape[2], input.shape[3])\n",
    "        (out_n, out_m) = ((n - k + 1 + s - 1) // s, (m - k + 1 + s - 1) // s)\n",
    "        assert out_n >= 0\n",
    "        assert out_m >= 0\n",
    "        out = np.zeros([batch_count, self.output_channels, out_n, out_m])\n",
    "        for batch_id in range(batch_count):\n",
    "            for out_ch in range(self.output_channels):\n",
    "                for x in range(0, n - k + 1, s):\n",
    "                    for y in range(0, m - k + 1, s):\n",
    "                        a = input[batch_id, :, x:x + k, y:y + k]\n",
    "                        b = self.kernel[:, :, :, out_ch]\n",
    "                        out[batch_id, out_ch, x // s, y // s] = dot(a, b) \\\n",
    "                            + self.bias[out_ch]\n",
    "        return out\n",
    "    def grad_x(self, input_data):\n",
    "        input = input_data.copy()\n",
    "        k = self.kernel_size\n",
    "        s = self.stride\n",
    "        (batch_count, input_channels) = (input.shape[0], input.shape[1])\n",
    "        output_channels = self.output_channels\n",
    "        (n, m) = (input.shape[2], input.shape[3])\n",
    "        (out_n, out_m) = ((n - k + 1 + s - 1) // s, (m - k + 1 + s - 1) // s)\n",
    "        out = np.zeros(shape = (batch_count, output_channels*out_n*out_m, input_channels*n*m))\n",
    "        for batch_id in range(batch_count):\n",
    "            for out_ch in range(self.output_channels):\n",
    "                for x in range(0, n - k + 1, s):\n",
    "                    for y in range(0, m - k + 1, s):\n",
    "                        for in_ch in range(input_channels):\n",
    "                            for nx in range(x,x+k):\n",
    "                                for ny in range(y,y+k):\n",
    "                                    dx, dy = nx-x, ny-y\n",
    "                                    id1 = out_ch*out_n*out_m + x*out_m + y\n",
    "                                    id2 = in_ch*n*m + nx*m + ny\n",
    "                                    out[batch_id, id1, id2] += self.kernel[dx, dy, in_ch, out_ch]\n",
    "        return out\n",
    "    def grad_kernel(self, input_data):\n",
    "        input = input_data.copy()\n",
    "        k = self.kernel_size\n",
    "        s = self.stride\n",
    "        (batch_count, input_channels) = (input.shape[0], input.shape[1])\n",
    "        output_channels = self.output_channels\n",
    "        (n, m) = (input.shape[2], input.shape[3])\n",
    "        (out_n, out_m) = ((n - k + 1 + s - 1) // s, (m - k + 1 + s - 1) // s)\n",
    "        out = np.zeros(shape = (batch_count, output_channels*out_n*out_m, k*k*input_channels*output_channels))\n",
    "        for batch_id in range(batch_count):\n",
    "            for out_ch in range(self.output_channels):\n",
    "                for x in range(0, n - k + 1, s):\n",
    "                    for y in range(0, m - k + 1, s):\n",
    "                        for in_ch in range(input_channels):\n",
    "                            for nx in range(x,x+k):\n",
    "                                for ny in range(y,y+k):\n",
    "                                    dx, dy = nx-x, ny-y\n",
    "                                    id1 = out_ch*out_n*out_m + x*out_m + y\n",
    "                                    id2 = dx*k*input_channels*output_channels + dy*input_channels*output_channels + in_ch*output_channels + out_ch\n",
    "                                    out[batch_id, id1, id2] += input[batch_id, in_ch, nx, ny]\n",
    "        return out\n",
    "    \n",
    "    def grad_bias(self, input_data):\n",
    "        input = input_data.copy()\n",
    "        k = self.kernel_size\n",
    "        s = self.stride\n",
    "        (batch_count, input_channels) = (input.shape[0], input.shape[1])\n",
    "        output_channels = self.output_channels\n",
    "        (n, m) = (input.shape[2], input.shape[3])\n",
    "        (out_n, out_m) = ((n - k + 1 + s - 1) // s, (m - k + 1 + s - 1) // s)\n",
    "        out = np.zeros(shape = (batch_count, output_channels*out_n*out_m, output_channels))\n",
    "        for batch_id in range(batch_count):\n",
    "            for out_ch in range(self.output_channels):\n",
    "                for x in range(0, n - k + 1, s):\n",
    "                    for y in range(0, m - k + 1, s):\n",
    "                        id1 = out_ch*out_n*out_m + x*out_m + y\n",
    "                        out[batch_id, id1, out_ch] += 1\n",
    "        return out\n",
    "        \n",
    "    def update_kernel(self, grad, learning_rate):\n",
    "        self.kernel -= learning_rate * np.mean(grad, axis=0).reshape(self.kernel.shape)\n",
    "    \n",
    "    def update_bias(self, grad,  learning_rate):\n",
    "        self.bias -= learning_rate * np.mean(grad, axis=0).reshape(self.bias.shape)\n",
    "        \n",
    "    def update_param(self, params_grad, learning_rate):\n",
    "        self.update_kernel(params_grad[0], learning_rate)\n",
    "        self.update_bias(params_grad[1], learning_rate)\n",
    "    \n",
    "    def grad_param(self, input_data):\n",
    "        return [self.grad_kernel(input_data), self.grad_bias(input_data)]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "lJeo-b3_R1Om"
   },
   "source": [
    "#### 1.4 Теперь настало время теста. \n",
    "#### Если вы всё сделали правильно, то запустив следующие ячейки у вас должна появиться надпись: Test PASSED\n",
    "\n",
    "Переходить к дальнейшим заданиям не имеем никакого смысла, пока вы не добьётесь прохождение теста\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "E-7IFb6rR1Om"
   },
   "source": [
    "#### Чтение данных"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "x55GwDyFR1On",
    "outputId": "3caaf27d-1c9b-487c-94c4-1999b0ae3e95"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(60000, 1, 28, 28) (60000, 10) (10000, 1, 28, 28) (10000, 10)\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "np.random.seed(123)  # for reproducibility\n",
    "from keras.utils import np_utils\n",
    "from keras.datasets import mnist\n",
    " \n",
    "(X_train, y_train), (X_test, y_test) = mnist.load_data()\n",
    "\n",
    "X_train = X_train.reshape(X_train.shape[0], 1, 28, 28)\n",
    "X_test = X_test.reshape(X_test.shape[0], 1, 28, 28)\n",
    "X_train = X_train.astype('float32')\n",
    "X_test = X_test.astype('float32')\n",
    "X_train /= 255\n",
    "X_test /= 255\n",
    " \n",
    "\n",
    "Y_train = np_utils.to_categorical(y_train, 10)\n",
    "Y_test = np_utils.to_categorical(y_test, 10)\n",
    "print(X_train.shape, Y_train.shape, X_test.shape, Y_test.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "-CUeQQ7YR1Oo"
   },
   "source": [
    "#### Подготовка моделей"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "id": "ez4H2EILR1Oo"
   },
   "outputs": [],
   "source": [
    "from keras.models import Sequential, Model\n",
    "from keras.layers import Dense, Dropout, Activation, Flatten, Input\n",
    "from keras.layers import Convolution2D, Conv2D, MaxPooling2D\n",
    "\n",
    "def get_keras_model():\n",
    "    input_image = Input(shape=(1, 28, 28))\n",
    "    pool1 = MaxPooling2D(pool_size=(2,2), data_format='channels_first')(input_image)\n",
    "    flatten = Flatten()(pool1)\n",
    "    dense1 = Dense(10, activation='softmax')(flatten)\n",
    "    model = Model(inputs=input_image, outputs=dense1)\n",
    "\n",
    "    from tensorflow.keras.optimizers import Adam, SGD\n",
    "    sgd = SGD(learning_rate=0.01, momentum=0.9, nesterov=True)\n",
    "    model.compile(loss='categorical_crossentropy',\n",
    "                  optimizer=sgd,\n",
    "                  metrics=['accuracy'])\n",
    "\n",
    "    history = model.fit(X_train, Y_train, validation_split=0.25, \n",
    "                        batch_size=32, epochs=2, verbose=1)\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "hH8brHETR1Oo"
   },
   "outputs": [],
   "source": [
    "def get_our_model(keras_model):\n",
    "    maxpool = MaxPooling()\n",
    "    flatten = FlattenLayer()\n",
    "    dense = DenseLayer(196, 10, W_init=keras_model.get_weights()[0],\n",
    "                       b_init=keras_model.get_weights()[1])\n",
    "    softmax = Softmax()\n",
    "    net = Network([maxpool, flatten, dense, softmax])\n",
    "    return net"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "qDc9ctbJR1Oo",
    "outputId": "8139af5a-33b3-4af5-e879-98c012b8833c"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/2\n",
      "1407/1407 [==============================] - 6s 4ms/step - loss: 0.5666 - accuracy: 0.8492 - val_loss: 0.3875 - val_accuracy: 0.8909\n",
      "Epoch 2/2\n",
      "1407/1407 [==============================] - 6s 4ms/step - loss: 0.3753 - accuracy: 0.8931 - val_loss: 0.3500 - val_accuracy: 0.8984\n"
     ]
    }
   ],
   "source": [
    "keras_model = get_keras_model()\n",
    "our_model = get_our_model(keras_model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "C01-fNUc1ukj"
   },
   "outputs": [],
   "source": [
    "our_model = get_our_model(keras_model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "Vbl-CIoLR1Op"
   },
   "outputs": [],
   "source": [
    "keras_prediction = keras_model.predict(X_test)\n",
    "our_model_prediction = our_model.predict(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "sI49RfwwR1Op",
    "outputId": "73e7f8ae-df64-4f08-ded1-e7a2a393cc04"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test PASSED\n"
     ]
    }
   ],
   "source": [
    "if np.sum(np.abs(keras_prediction - our_model_prediction)) < 0.01:\n",
    "    print('Test PASSED')\n",
    "else:\n",
    "    print('Something went wrong!')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "r9EzhAa9R1Op"
   },
   "source": [
    "### 2. Вычисление производных по входу для слоёв нейронной сети"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "_HN3iyKDR1Op"
   },
   "source": [
    "#### 2.1 (1 балл) Реализуйте метод forward для класса CrossEntropy\n",
    "Напоминание: $$ crossentropy = L(p, y) =  - \\sum\\limits_i y_i log p_i, $$\n",
    "где вектор $(p_1, ..., p_k) $ -  выход классификационного алгоритма, а $(y_1,..., y_k)$ - правильные метки класса в унарной кодировке (one-hot encoding)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "id": "DNSfBzAUR1Oq"
   },
   "outputs": [],
   "source": [
    "class CrossEntropy(object):\n",
    "    def __init__(self, eps=0.00001):\n",
    "        self.name = 'CrossEntropy'\n",
    "        self.eps = eps\n",
    "    \n",
    "    def forward(self, input_data, labels):\n",
    "        out = np.zeros(shape = input_data.shape[0])\n",
    "        for batch_id in range(input_data.shape[0]):\n",
    "            for i in range(input_data.shape[1]):\n",
    "                out[batch_id] -= labels[batch_id, i] * np.log(input_data[batch_id, i]+self.eps)\n",
    "        return out\n",
    "    \n",
    "    def calculate_loss(self,input_data, labels):\n",
    "        return self.forward(input_data, labels)\n",
    "    \n",
    "    def grad_x(self, input_data, labels):\n",
    "        out = np.zeros_like(input_data, dtype = np.float64)\n",
    "        for batch_id in range(input_data.shape[0]):\n",
    "            for i in range(input_data.shape[1]):\n",
    "                out[batch_id, i] = (-labels[batch_id, i] / (input_data[batch_id, i]+self.eps))\n",
    "        return out"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "rNOY8iw7R1Oq"
   },
   "source": [
    "#### 2.2 (2 баллa) Реализуйте метод grad_x класса CrossEntropy, который возвращает $\\frac{\\partial L}{\\partial p}$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "AfSP4v2VR1Oq"
   },
   "source": [
    "Проверить работоспособность кода поможет следующий тест:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "xfaOsOe4R1Oq",
    "outputId": "b6aff110-c21d-427d-ad78-b505d04639af"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test PASSED\n"
     ]
    }
   ],
   "source": [
    "def numerical_diff_net(net, x, labels):\n",
    "    eps = 0.00001\n",
    "    right_answer = []\n",
    "    for i in range(len(x[0])):\n",
    "        delta = np.zeros(len(x[0]))\n",
    "        delta[i] = eps\n",
    "        diff = (net.calculate_loss(x + delta, labels) - net.calculate_loss(x-delta, labels)) / (2*eps)\n",
    "        right_answer.append(diff)\n",
    "    return np.array(right_answer).T\n",
    "\n",
    "def test_net(net):\n",
    "    x = np.array([[1, 2, 3], [2, 3, 4]])\n",
    "    labels = np.array([[0.3, 0.2, 0.5], [0.3, 0.2, 0.5]])\n",
    "    num_grad = numerical_diff_net(net, x, labels)\n",
    "    grad = net.grad_x(x, labels)\n",
    "    if np.sum(np.abs(num_grad - grad)) < 0.01:\n",
    "        print('Test PASSED')\n",
    "    else:\n",
    "        print('Something went wrong!')\n",
    "        print('Numerical grad is')\n",
    "        print(num_grad)\n",
    "        print('Your gradiend is ')\n",
    "        print(grad)\n",
    "        \n",
    "loss = CrossEntropy()\n",
    "test_net(loss)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "mas1G_6gR1Oq"
   },
   "source": [
    "#### 2.3 (2 балла)   Реализуйте метод grad_x класса Softmax, который возвращает $\\frac{\\partial Softmax}{\\partial x}$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "a-pJuMD-R1Or"
   },
   "source": [
    "Проверить работоспособность кода поможет следующий тест:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "ZthzlKUcR1Or",
    "outputId": "38694f62-4734-4672-8d8c-8f5617bee2fa"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test PASSED\n"
     ]
    }
   ],
   "source": [
    "def numerical_diff_layer(layer, x):\n",
    "    eps = 0.00001\n",
    "    right_answer = []\n",
    "    for i in range(len(x[0])):\n",
    "        delta = np.zeros(len(x[0]))\n",
    "        delta[i] = eps\n",
    "        diff = (layer.forward(x + delta) - layer.forward(x-delta)) / (2*eps)\n",
    "        right_answer.append(diff.T)\n",
    "    return np.array(right_answer).T\n",
    "\n",
    "def test_layer(layer):\n",
    "    x = np.array([[1, 2, 3], [2, -3, 4]])\n",
    "    num_grad = numerical_diff_layer(layer, x)\n",
    "    grad = layer.grad_x(x)\n",
    "    if np.sum(np.abs(num_grad - grad)) < 0.01:\n",
    "        print('Test PASSED')\n",
    "    else:\n",
    "        print('Something went wrong!')\n",
    "        print('Numerical grad is')\n",
    "        print(num_grad)\n",
    "        print('Your gradiend is ')\n",
    "        print(grad)\n",
    "        \n",
    "layer = Softmax()\n",
    "test_layer(layer)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "_Jn587J6R1Or"
   },
   "source": [
    "#### 2.4 (5 баллов) Реализуйте метод grad_x для классов ReLU и DenseLayer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "0fByOumwR1Or",
    "outputId": "1fbc3516-e963-4c74-b090-fe8c40c647de"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test PASSED\n"
     ]
    }
   ],
   "source": [
    "layer = ReLU()\n",
    "test_layer(layer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "TWE6_F2YR1Or",
    "outputId": "dcbcb07f-494e-4050-ee37-aa270f343f24"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test PASSED\n"
     ]
    }
   ],
   "source": [
    "layer = DenseLayer(3,4)\n",
    "test_layer(layer)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "9XFt2qXYR1Os"
   },
   "source": [
    "#### 2.5 (4 балла) Для класса Network реализуйте метод grad_x, который должен реализовывать взятие производной от лосса по входу"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "h5htmL9lR1Os",
    "outputId": "f31240ed-94a1-49c4-eabc-6684543631c4"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test PASSED\n"
     ]
    }
   ],
   "source": [
    "net = Network([DenseLayer(3, 10), ReLU(), DenseLayer(10, 3), Softmax()], loss=CrossEntropy())\n",
    "test_net(net)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "9LNnSrjBR1Os"
   },
   "source": [
    "### 3. Реализация градиентов по параметрам и метода обратного распространения ошибки с обновлением парметров сети"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "UWPXT9EmR1Os"
   },
   "source": [
    "#### 3.1 (4 балла) Реализуйте функции grad_b и grad_W. При подготовке теста grad_W предполагается, что W является отномерным вектором."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "QgWhlycoR1Os",
    "outputId": "dec834ff-aa1b-457e-aaba-7e0df40c5d19"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test PASSED\n"
     ]
    }
   ],
   "source": [
    "def numerical_grad_b(input_size, output_size, b, W, x):\n",
    "    eps = 0.00001\n",
    "    right_answer = []\n",
    "    for i in range(len(b)):\n",
    "        delta = np.zeros(b.shape)\n",
    "        delta[i] = eps\n",
    "        dense1 = DenseLayer(input_size, output_size, W_init=W, b_init=b+delta)\n",
    "        dense2 = DenseLayer(input_size, output_size, W_init=W, b_init=b-delta)\n",
    "        diff = (dense1.forward(x) - dense2.forward(x)) / (2*eps)\n",
    "        right_answer.append(diff.T)\n",
    "    return np.array(right_answer).T\n",
    "\n",
    "def test_grad_b():\n",
    "    input_size = 3\n",
    "    output_size = 4 \n",
    "    W_init = np.random.random((input_size, output_size))\n",
    "    b_init = np.random.random((output_size,))\n",
    "    x = np.random.random((2, input_size))\n",
    "    \n",
    "    dense = DenseLayer(input_size, output_size, W_init, b_init)\n",
    "    grad = dense.grad_b(x)\n",
    "\n",
    "    num_grad = numerical_grad_b(input_size, output_size, b_init, W_init, x)\n",
    "    if np.sum(np.abs(num_grad - grad)) < 0.01:\n",
    "        print('Test PASSED')\n",
    "    else:\n",
    "        print('Something went wrong!')\n",
    "        print('Numerical grad is')\n",
    "        print(num_grad)\n",
    "        print('Your gradiend is ')\n",
    "        print(grad)\n",
    "\n",
    "test_grad_b()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "zzi0e9KtR1Ov",
    "outputId": "1c05ae23-bf9c-4ec5-edd5-9fc18b63d809"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test PASSED\n"
     ]
    }
   ],
   "source": [
    "def numerical_grad_W(input_size, output_size, b, W, x):\n",
    "    eps = 0.00001\n",
    "    right_answer = []\n",
    "    for i in range(W.shape[0]):\n",
    "        for j in range(W.shape[1]):\n",
    "            delta = np.zeros(W.shape)\n",
    "            delta[i, j] = eps\n",
    "            dense1 = DenseLayer(input_size, output_size, W_init=W+delta, b_init=b)\n",
    "            dense2 = DenseLayer(input_size, output_size, W_init=W-delta, b_init=b)\n",
    "            diff = (dense1.forward(x) - dense2.forward(x)) / (2*eps)\n",
    "            right_answer.append(diff.T)\n",
    "    return np.array(right_answer).T\n",
    "\n",
    "def test_grad_W():\n",
    "    input_size = 3\n",
    "    output_size = 4 \n",
    "    W_init = np.random.random((input_size, output_size))\n",
    "    b_init = np.random.random((4,))\n",
    "    x = np.random.random((2, input_size))\n",
    "        \n",
    "    dense = DenseLayer(input_size, output_size, W_init, b_init)\n",
    "    grad = dense.grad_W(x)\n",
    "\n",
    "    num_grad = numerical_grad_W(input_size, output_size, b_init, W_init, x)\n",
    "    if np.sum(np.abs(num_grad - grad)) < 0.01:\n",
    "        print('Test PASSED')\n",
    "    else:\n",
    "        print('Something went wrong!')\n",
    "        print('Numerical grad is')\n",
    "        print(num_grad)\n",
    "        print('Your gradiend is ')\n",
    "        print(grad)\n",
    "\n",
    "test_grad_W()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "VP92HI7gR1Ov"
   },
   "source": [
    "#### 3.2 (4 балла) Полностью реализуйте метод обратного распространения ошибки в функции train_step класса Network\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "hDYqkDYPR1Ov"
   },
   "source": [
    "Рекомендуем реализовать сначала функцию Network.grad_param(), которая возвращает список длиной в количество слоёв и элементом которого является список градиентов по параметрам.\n",
    "После чего, имея список градиентов, написать функцию обновления параметров для каждого слоя. \n",
    "\n",
    "Совет: рекомендуем написать тест для кода подсчета градиента по параметрам, чтобы быть уверенным в том, что градиент через всю сеть считается правильно\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "h6dDBasZR1Ov"
   },
   "source": [
    "#### 3.3 Ознакомьтесь с реализацией функции fit класса Network. Запустите обучение модели. Если всё работает правильно, то точность на валидации должна будет возрастать"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "oFb7tS7nSAMi",
    "outputId": "a682ac33-2c8b-4813-9d6d-4a36603d5713"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 937/937 [03:25<00:00,  4.57it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      " 1 epoch: val 0.85 \n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 937/937 [03:24<00:00,  4.58it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      " 2 epoch: val 0.87 \n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 937/937 [03:24<00:00,  4.59it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      " 3 epoch: val 0.88 \n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 937/937 [03:23<00:00,  4.60it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      " 4 epoch: val 0.89 \n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 937/937 [03:24<00:00,  4.58it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      " 5 epoch: val 0.89 \n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "net = Network([DenseLayer(784, 10), Softmax()], loss=CrossEntropy())\n",
    "trainX = X_train.reshape(len(X_train), -1)\n",
    "net.fit(trainX[::3], Y_train[::3], validation_split=0.25, \n",
    "            batch_size=16, nb_epoch=5, learning_rate=0.01)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "ZJc-ZGfHUfeM",
    "outputId": "4d9542fd-b3de-4863-9913-d04e1183e445"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 468/468 [03:06<00:00,  2.51it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      " 1 epoch: val 0.81 \n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 468/468 [03:07<00:00,  2.49it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      " 2 epoch: val 0.85 \n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 468/468 [03:07<00:00,  2.50it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      " 3 epoch: val 0.87 \n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 468/468 [03:07<00:00,  2.50it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      " 4 epoch: val 0.88 \n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 468/468 [03:07<00:00,  2.50it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      " 5 epoch: val 0.88 \n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "net = Network([DenseLayer(784, 20), ReLU(), DenseLayer(20, 10), Softmax()], loss=CrossEntropy())\n",
    "trainX = X_train.reshape(len(X_train), -1)\n",
    "net.fit(trainX[::6], Y_train[::6], validation_split=0.25, \n",
    "            batch_size=16, nb_epoch=5, learning_rate=0.01)    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "44tBMzcgSC1L",
    "outputId": "d9c5f627-632b-4e5e-c0ba-3cb50947c48c"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 468/468 [03:03<00:00,  2.55it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      " 1 epoch: val 0.78 \n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 468/468 [03:02<00:00,  2.56it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      " 2 epoch: val 0.84 \n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 468/468 [03:03<00:00,  2.55it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      " 3 epoch: val 0.86 \n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 468/468 [03:03<00:00,  2.55it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      " 4 epoch: val 0.87 \n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 468/468 [03:02<00:00,  2.56it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      " 5 epoch: val 0.88 \n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "net = Network([DenseLayer(784, 20), ReLU(), DenseLayer(20, 10), Softmax()], loss=CrossEntropy())\n",
    "trainX = X_train.reshape(len(X_train), -1)\n",
    "net.fit(trainX[::6], Y_train[::6], validation_split=0.25, \n",
    "            batch_size=16, nb_epoch=5, learning_rate=0.01)    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "hKA17BdXR1Ow"
   },
   "source": [
    "#### 3.5 (2 балла) Продемонстрируйте, что ваша реализация позволяет обучать более глубокие нейронные сети "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "vEMGGEviG0B4",
    "outputId": "073263a3-737a-40a2-8437-3f5ed61c3ee6"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 468/468 [13:04<00:00,  1.68s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      " 1 epoch: val 0.81 \n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 468/468 [13:05<00:00,  1.68s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      " 2 epoch: val 0.86 \n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 468/468 [13:05<00:00,  1.68s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      " 3 epoch: val 0.88 \n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 468/468 [13:05<00:00,  1.68s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      " 4 epoch: val 0.89 \n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 468/468 [13:06<00:00,  1.68s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      " 5 epoch: val 0.90 \n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "net = Network([DenseLayer(784, 80), ReLU(), DenseLayer(80, 40), ReLU(), DenseLayer(40, 20), ReLU(), DenseLayer(20, 10), Softmax()], loss=CrossEntropy())\n",
    "trainX = X_train.reshape(len(X_train), -1)\n",
    "net.fit(trainX[::6], Y_train[::6], validation_split=0.25, \n",
    "            batch_size=16, nb_epoch=5, learning_rate=0.01)    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "_95eLeRTIziW"
   },
   "source": [
    "Я решил реализовать градиенты для слоя свертки, а также обратное распространение ошибки для сети, для слоев свертки.\n",
    "Сначала я сделал это, считая grad_x, также grad_bias и grad_kernel. \n",
    "Также я добавил в слой ReLU вариант grad_x, когда на вход подают четырехмерный тензор.\n",
    "\n",
    "Код, который был написан для этого - это все, что сверху.\n",
    "\n",
    "Потом я заметил, что если я ставлю сеть с большим количество сверток, то заканчивается ОЗУ.\n",
    "Причина была в том, что grad_x для слоя свертки имел очень большой размер, когда сверток много.\n",
    "\n",
    "Я вот рассказал эту проблему Вам на паре. Одно из решений - это использовать разреженные матрицы. Но, зная локальность свертки, я подумал, что может быть, зная градиент следующего слоя, можно пересчитать градиент текущего без перемножения матриц конкретно для слоев свертки (и также макс-пулинг изменил потом).\n",
    "\n",
    "Пусть у нас есть dLoss/d(выхода слоя свертки) и нам надо посчитать\n",
    "dLoss/d(вход слоя свертки).\n",
    "Для каждого выхода мы знали, какие элементы входа участвовали для его получения (окно свертки) - тогда зная градиент конкретного выхода, можно было прибавить его, домноженный на соответствующий коэффициент ядра в градиент входа.\n",
    "Получается, что это что-то похожее на транспонированную свертку... Я не смог это себе представить, как будет выглядеть в операциях свертки, но написал вот то, что описывал - \"зная градиент конкретного выхода, можно было прибавить его, домноженный на соответствующий коэффициент ядра в градиент входа\".\n",
    "\n",
    "Это позволило решить проблему с ОЗУ, так как я избавился от матричного умножения в слоях свертки.\n",
    "\n",
    "Для того, чтобы это было возможно - я в метод grad_param не вычисляю grad_x, а запоминаю вход, который был для конкретного слоя. Также для слоя ReLU я не вычисляю grad_x, а проделываю вручную - в элементах, где на входе был отрицательный элемент - я зануляю градиент."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "id": "PY1AycMeCsTn"
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from sklearn.model_selection import train_test_split\n",
    "from tqdm import tqdm\n",
    "\n",
    "class Network(object):\n",
    "    def __init__(self, layers, loss=None):\n",
    "        self.name = 'Network'\n",
    "        self.layers = layers\n",
    "        self.loss = loss\n",
    "    \n",
    "    def forward(self, input_data):\n",
    "        return self.predict(input_data)\n",
    "\n",
    "    def grad_x(self, input_data, labels):\n",
    "        gradients_x = []\n",
    "        current_input = input_data\n",
    "        for layer in self.layers:\n",
    "            gradients_x.append(layer.grad_x(current_input))\n",
    "            current_input = layer.forward(current_input) \n",
    "        gradient_x = self.loss.grad_x(current_input, labels)\n",
    "        gradient_x = np.reshape(gradient_x, (gradient_x.shape[0], 1, gradient_x.shape[1]))\n",
    "        for gradient_x_layer in reversed(gradients_x):\n",
    "            gradient_x = np.matmul(gradient_x, gradient_x_layer)\n",
    "        gradient_x = np.reshape(gradient_x, (gradient_x.shape[0], gradient_x.shape[2]))\n",
    "        return gradient_x\n",
    "\n",
    "    def grad_param(self, input_data, labels):\n",
    "        gradients_param = []\n",
    "        current_input = input_data\n",
    "        for layer in self.layers:\n",
    "            gradients_param.append([current_input.copy(), layer.grad_param(current_input)])\n",
    "            current_input = layer.forward(current_input) \n",
    "        gradients_param.append([self.loss.grad_x(current_input, labels), []])\n",
    "        return gradients_param\n",
    "\n",
    "    def update(self, grad_list, learning_rate):\n",
    "        gradient_x = grad_list[-1][0]\n",
    "        gradient_x = np.reshape(gradient_x, (gradient_x.shape[0], 1, gradient_x.shape[1]))\n",
    "        for i in range(len(grad_list)-2, -1, -1):\n",
    "            layer = self.layers[i]\n",
    "            if layer.name == 'Dense':\n",
    "                layer.update_param([np.matmul(gradient_x, grad_list[i][1][0]), np.matmul(gradient_x, grad_list[i][1][1])], learning_rate)\n",
    "                gradient_x = np.matmul(gradient_x, layer.grad_x(grad_list[i][0]))\n",
    "            elif layer.name == 'Softmax':\n",
    "                gradient_x = np.matmul(gradient_x, layer.grad_x(grad_list[i][0]))\n",
    "            elif layer.name == 'MaxPooling':\n",
    "                gradient_x = layer.grad_loss_x(grad_list[i][0], gradient_x)\n",
    "            elif layer.name == 'ReLU':\n",
    "                input_image = grad_list[i][0]\n",
    "                input_image[input_image<0] = 0\n",
    "                input_image[input_image>=0] = 1\n",
    "                input_image = np.reshape(input_image, gradient_x.shape)\n",
    "                gradient_x = gradient_x * input_image\n",
    "            elif layer.name == 'Conv2D':\n",
    "                layer.update_param([np.matmul(gradient_x, grad_list[i][1][0]), np.matmul(gradient_x, grad_list[i][1][1])], learning_rate)\n",
    "                gradient_x = layer.grad_loss_x(grad_list[i][0], gradient_x)\n",
    "            elif layer.name == 'Flatten':\n",
    "                gradient_x = gradient_x #nothing changed, gradient_x * E\n",
    "            else:\n",
    "                print(\"BAD\")\n",
    "    \n",
    "    def predict(self, input_data):\n",
    "        current_input = input_data\n",
    "        for layer in self.layers:\n",
    "            current_input = layer.forward(current_input)     \n",
    "        return current_input\n",
    "    \n",
    "    def calculate_loss(self, input_data, labels):\n",
    "        return self.loss.forward(self.predict(input_data), labels)\n",
    "    \n",
    "    def train_step(self, input_data, labels, learning_rate=0.001):\n",
    "        grad_list = self.grad_param(input_data, labels)\n",
    "        self.update(grad_list, learning_rate)\n",
    "    \n",
    "    def fit(self, trainX, trainY, validation_split=0.25, \n",
    "            batch_size=1, nb_epoch=1, learning_rate=0.01):\n",
    "        \n",
    "        train_x, val_x, train_y, val_y = train_test_split(trainX, trainY, \n",
    "                                                          test_size=validation_split,\n",
    "                                                          random_state=42)\n",
    "        for epoch in range(nb_epoch):\n",
    "            #train one epoch\n",
    "            for i in tqdm(range(int(len(train_x)/batch_size))):\n",
    "                batch_x = train_x[i*batch_size: (i+1)*batch_size]\n",
    "                batch_y = train_y[i*batch_size: (i+1)*batch_size]\n",
    "                self.train_step(batch_x, batch_y, learning_rate)\n",
    "            #validate\n",
    "            val_accuracy = self.evaluate(val_x, val_y)\n",
    "            print(\"\\n\",'%d epoch: val %.2f' %(epoch+1, val_accuracy), \"\\n\")\n",
    "            \n",
    "    def evaluate(self, testX, testY):\n",
    "        y_pred = np.argmax(self.predict(testX), axis=1)            \n",
    "        y_true = np.argmax(testY, axis=1)\n",
    "        val_accuracy = np.sum((y_pred == y_true))/(len(y_true))\n",
    "        return val_accuracy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "id": "PHdZUNIIbmAv"
   },
   "outputs": [],
   "source": [
    "class DenseLayer(Layer):\n",
    "    def __init__(self, input_dim, output_dim, W_init=None, b_init=None):\n",
    "        self.name = 'Dense'\n",
    "        self.input_dim = input_dim\n",
    "        self.output_dim = output_dim\n",
    "        if W_init is None or b_init is None:\n",
    "            self.W = np.random.normal(0, np.sqrt(2. / input_dim), size = (input_dim, output_dim))\n",
    "            self.b = np.zeros(output_dim, 'float32')\n",
    "        else:\n",
    "            self.W = W_init\n",
    "            self.b = b_init\n",
    "    def forward(self, input_data):\n",
    "        assert input_data.shape[1] == self.W.shape[0]\n",
    "        out = np.matmul(input_data, self.W) + self.b\n",
    "        return out\n",
    "    def grad_x(self, input_data):\n",
    "        (input_dim, output_dim) = (self.input_dim, self.output_dim)\n",
    "        out = np.zeros(shape = (input_data.shape[0], output_dim, input_dim))\n",
    "        for batch_id in range(input_data.shape[0]):\n",
    "            array = input_data[batch_id]\n",
    "            for out_id in range(output_dim):\n",
    "                for in_id in range(input_dim):\n",
    "                    out[batch_id, out_id, in_id] = self.W[in_id, out_id]\n",
    "        return out\n",
    "    def grad_b(self, input_data):\n",
    "        (input_dim, output_dim) = (self.input_dim, self.output_dim)\n",
    "        out = np.zeros(shape = (input_data.shape[0], output_dim, output_dim))\n",
    "        for batch_id in range(input_data.shape[0]):\n",
    "            for out_id in range(output_dim):\n",
    "                out[batch_id, out_id, out_id] = 1\n",
    "        return out\n",
    "    def grad_W(self, input_data):\n",
    "        (input_dim, output_dim) = (self.input_dim, self.output_dim)\n",
    "        out = np.zeros(shape = (input_data.shape[0], output_dim, input_dim * output_dim))\n",
    "        for batch_id in range(input_data.shape[0]):\n",
    "            for k in range(output_dim):\n",
    "                # d out_j / d w_s_t, if j != t zero\n",
    "                for j in range(input_dim):\n",
    "                    out[batch_id, k, j*output_dim + k] = input_data[batch_id, j]\n",
    "        return out\n",
    "    \n",
    "    def update_W(self, grad, learning_rate):\n",
    "        self.W -= learning_rate * np.mean(grad, axis=0).reshape(self.W.shape)\n",
    "    \n",
    "    def update_b(self, grad,  learning_rate):\n",
    "        self.b -= learning_rate * np.mean(grad, axis=0).reshape(self.b.shape)\n",
    "        \n",
    "    def update_param(self, params_grad, learning_rate):\n",
    "        self.update_W(params_grad[0], learning_rate)\n",
    "        self.update_b(params_grad[1], learning_rate)\n",
    "    \n",
    "    def grad_param(self, input_data):\n",
    "        return [self.grad_W(input_data), self.grad_b(input_data)]\n",
    "    \n",
    "\n",
    "class ReLU(Layer):\n",
    "    def __init__(self):\n",
    "        self.name = 'ReLU'\n",
    "    def forward(self, input_data):\n",
    "        out = input_data.copy()\n",
    "        out[out < 0] = 0\n",
    "        return out\n",
    "    def grad_x(self, input_data):\n",
    "        if len(input_data.shape) == 2:\n",
    "            (batch_size, logits) = input_data.shape\n",
    "            out = np.zeros(shape = (batch_size, logits, logits))\n",
    "            for batch_id in range(batch_size):\n",
    "                for i in range(logits):\n",
    "                    if input_data[batch_id, i] > 0:\n",
    "                        out[batch_id, i, i] = 1\n",
    "            return out\n",
    "        batch_size = input_data.shape[0]\n",
    "        input_channels = input_data.shape[1]\n",
    "        n = input_data.shape[2]\n",
    "        m = input_data.shape[3]\n",
    "        out = np.zeros(shape = (batch_size, input_channels*n*m, input_channels*n*m))\n",
    "        for batch_id in range(batch_size):\n",
    "            for in_ch in range(input_channels):\n",
    "                for x in range(n):\n",
    "                    for y in range(m):\n",
    "                        if input_data[batch_id, in_ch, x, y] > 0:\n",
    "                            out[batch_id, in_ch*n*m+x*m+y, in_ch*n*m+x*m+y] = 1\n",
    "        return out\n",
    "    \n",
    "    \n",
    "class Softmax(Layer):\n",
    "    def __init__(self):\n",
    "        self.name = 'Softmax'\n",
    "    def forward(self, input_data):\n",
    "        def g(array):\n",
    "            array = np.exp(array)\n",
    "            array /= np.sum(array)\n",
    "            return array\n",
    "        return np.apply_along_axis(g, -1, input_data)\n",
    "    def grad_x(self, input_data):\n",
    "        (batch_size, logits) = input_data.shape\n",
    "        out = np.zeros(shape = (batch_size, logits, logits))\n",
    "        for batch_id in range(batch_size):\n",
    "            array = np.exp(input_data[batch_id])\n",
    "            sum_of_array = np.sum(array)\n",
    "            for i in range(logits):\n",
    "                for j in range(logits):\n",
    "                    out[batch_id, i, j] = - (array[i] * array[j]) / sum_of_array**2\n",
    "                    if i == j:\n",
    "                         out[batch_id, i, j] += array[i] / sum_of_array\n",
    "\n",
    "        return out\n",
    "    \n",
    "\n",
    "class FlattenLayer(Layer):\n",
    "    def __init__(self):\n",
    "        self.name = 'Flatten'\n",
    "    def forward(self, input_data):\n",
    "        length = input_data.shape[1] * input_data.shape[2] \\\n",
    "            * input_data.shape[3]\n",
    "        out = np.zeros((input_data.shape[0], length))\n",
    "        for batch_id in range(input_data.shape[0]):\n",
    "            out[batch_id] = np.transpose(input_data[batch_id], axes = (1, 2, 0)).reshape(length)\n",
    "        return out\n",
    "    def grad_x(self, input_data):\n",
    "        length = input_data.shape[1] * input_data.shape[2] \\\n",
    "            * input_data.shape[3]\n",
    "        out = np.zeros((input_data.shape[0], length, length))\n",
    "        for batch_id in range(input_data.shape[0]):\n",
    "            for i in range(length):\n",
    "                out[batch_id, i, i] = 1\n",
    "        return out\n",
    "\n",
    "\n",
    "class MaxPooling(Layer):\n",
    "    def __init__(self, pool_size=(2, 2), strides=2):\n",
    "        self.name = 'MaxPooling'\n",
    "        self.pool_size = pool_size\n",
    "        self.strides = 2\n",
    "    def forward(self, input_data):\n",
    "        input = input_data.copy()\n",
    "        (k1, k2) = self.pool_size\n",
    "        s = self.strides\n",
    "        (batch_count, channels_count) = (input.shape[0], input.shape[1])\n",
    "        (n, m) = (input.shape[2], input.shape[3])\n",
    "        (out_n, out_m) = ((n - k1 + 1 + s - 1) // s, (m - k2 + 1 + s - 1) // s)\n",
    "        assert out_n >= 0\n",
    "        assert out_m >= 0\n",
    "        out = np.zeros([batch_count, channels_count, out_n, out_m])\n",
    "        for x in range(0, n - k1 + 1, s):\n",
    "            for y in range(0, m - k2 + 1, s):\n",
    "                a = input[:, :, x:x + k1, y:y + k2]\n",
    "                out[:, :, x // s, y // s] = np.amax(a, axis =(2,3))\n",
    "        return out    \n",
    "    def grad_x(self, input_data):\n",
    "        input = input_data.copy()\n",
    "        (k1, k2) = self.pool_size\n",
    "        s = self.strides\n",
    "        (batch_count, channels_count) = (input.shape[0], input.shape[1])\n",
    "        (n, m) = (input.shape[2], input.shape[3])\n",
    "        (out_n, out_m) = ((n - k1 + 1 + s - 1) // s, (m - k2 + 1 + s - 1) // s)\n",
    "        assert out_n >= 0\n",
    "        assert out_m >= 0\n",
    "        out = np.zeros([batch_count, channels_count*out_n*out_m, channels_count*n*m])\n",
    "        for batch_id in range(batch_count):\n",
    "            for out_ch in range(channels_count):\n",
    "                for x in range(0, n - k1 + 1, s):\n",
    "                    for y in range(0, m - k2 + 1, s):\n",
    "                        mymax = np.amax(input[batch_id, out_ch, x:x+k1, y:y+k2])\n",
    "                        for nx in range(x,x+k1):\n",
    "                            for ny in range(y,y+k2):\n",
    "                                if input[batch_id, out_ch, nx, ny] == mymax:\n",
    "                                    id1 = out_ch*out_n*out_m + (x//s)*out_m + (y//s)\n",
    "                                    id2 = out_ch*n*m + nx*m + ny\n",
    "                                    out[batch_id, id1, id2] = 1\n",
    "        return out  \n",
    "    def grad_loss_x(self, input_data, grad_loss_nxt):\n",
    "        input = input_data.copy()\n",
    "        (k1, k2) = self.pool_size\n",
    "        s = self.strides\n",
    "        (batch_count, channels_count) = (input.shape[0], input.shape[1])\n",
    "        (n, m) = (input.shape[2], input.shape[3])\n",
    "        (out_n, out_m) = ((n - k1 + 1 + s - 1) // s, (m - k2 + 1 + s - 1) // s)\n",
    "        out = np.zeros([batch_count, 1, channels_count*n*m])\n",
    "        for batch_id in range(batch_count):\n",
    "            for out_ch in range(channels_count):\n",
    "                for x in range(0, n - k1 + 1, s):\n",
    "                    for y in range(0, m - k2 + 1, s):\n",
    "                        mymax = np.amax(input[batch_id, out_ch, x:x+k1, y:y+k2])\n",
    "                        cntmx = 0\n",
    "                        for nx in range(x,x+k1):\n",
    "                            for ny in range(y,y+k2):\n",
    "                                if input[batch_id, out_ch, nx, ny] == mymax:\n",
    "                                    cntmx += 1\n",
    "                        for nx in range(x,x+k1):\n",
    "                            for ny in range(y,y+k2):\n",
    "                                if input[batch_id, out_ch, nx, ny] == mymax:\n",
    "                                    id1 = out_ch*out_n*out_m + (x//s)*out_m + (y//s)\n",
    "                                    id2 = out_ch*n*m + nx*m + ny\n",
    "                                    out[batch_id, 0, id2] = 1/cntmx * grad_loss_nxt[batch_id, 0, id1]\n",
    "        return out  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "id": "QZYMoZe5bo9N"
   },
   "outputs": [],
   "source": [
    "class Conv2DLayer(Layer):\n",
    "    def __init__(self, kernel_size=3, input_channels=2, output_channels=3, \n",
    "                 stride=1, K_init=None, b_init=None):\n",
    "      # padding: 'valid'\n",
    "      # Работаем с квадратными ядрами, поэтому kernel_size - одно число\n",
    "      # Работаем с единообразным сдвигом, поэтому stride - одно число\n",
    "      # Фильтр размерности [kernel_size, kernel_size, input_channels, output_channels]\n",
    "      self.name = 'Conv2D'\n",
    "      self.kernel_size = kernel_size\n",
    "      self.input_channels = input_channels\n",
    "      self.output_channels = output_channels\n",
    "      self.stride = stride\n",
    "      if K_init is None or b_init is None:\n",
    "          self.kernel = np.random.normal(0, np.sqrt(2. / (kernel_size*kernel_size*output_channels)), size =  (kernel_size,kernel_size,input_channels,output_channels))\n",
    "          self.bias = np.zeros(output_channels, 'float32')\n",
    "      else:\n",
    "          self.kernel = K_init\n",
    "          self.bias = b_init\n",
    "            \n",
    "    def forward(self, input_data):\n",
    "        def dot(a, b):\n",
    "            a = np.moveaxis(a, 0, -1)\n",
    "            assert a.shape == b.shape\n",
    "            return (a*b).sum()\n",
    "\n",
    "        input = input_data.copy()\n",
    "        assert self.input_channels == input.shape[1]\n",
    "        k = self.kernel_size\n",
    "        s = self.stride\n",
    "        (batch_count, channels_count) = (input.shape[0], input.shape[1])\n",
    "        (n, m) = (input.shape[2], input.shape[3])\n",
    "        (out_n, out_m) = ((n - k + 1 + s - 1) // s, (m - k + 1 + s - 1) // s)\n",
    "        assert out_n >= 0\n",
    "        assert out_m >= 0\n",
    "        out = np.zeros([batch_count, self.output_channels, out_n, out_m])\n",
    "        for batch_id in range(batch_count):\n",
    "            for out_ch in range(self.output_channels):\n",
    "                for x in range(0, n - k + 1, s):\n",
    "                    for y in range(0, m - k + 1, s):\n",
    "                        a = input[batch_id, :, x:x + k, y:y + k]\n",
    "                        b = self.kernel[:, :, :, out_ch]\n",
    "                        out[batch_id, out_ch, x // s, y // s] = dot(a, b) \\\n",
    "                            + self.bias[out_ch]\n",
    "        return out\n",
    "    def grad_x(self, input_data):\n",
    "        input = input_data.copy()\n",
    "        k = self.kernel_size\n",
    "        s = self.stride\n",
    "        (batch_count, input_channels) = (input.shape[0], input.shape[1])\n",
    "        output_channels = self.output_channels\n",
    "        (n, m) = (input.shape[2], input.shape[3])\n",
    "        (out_n, out_m) = ((n - k + 1 + s - 1) // s, (m - k + 1 + s - 1) // s)\n",
    "        out = np.zeros(shape = (batch_count, output_channels*out_n*out_m, input_channels*n*m))\n",
    "        for batch_id in range(batch_count):\n",
    "            for out_ch in range(self.output_channels):\n",
    "                for x in range(0, n - k + 1, s):\n",
    "                    for y in range(0, m - k + 1, s):\n",
    "                        for in_ch in range(input_channels):\n",
    "                            for nx in range(x,x+k):\n",
    "                                for ny in range(y,y+k):\n",
    "                                    dx, dy = nx-x, ny-y\n",
    "                                    id1 = out_ch*out_n*out_m + x*out_m + y\n",
    "                                    id2 = in_ch*n*m + nx*m + ny\n",
    "                                    out[batch_id, id1, id2] += self.kernel[dx, dy, in_ch, out_ch]\n",
    "        return out\n",
    "    def grad_loss_x(self, input_data, grad_loss_nxt):\n",
    "        input = input_data.copy()\n",
    "        k = self.kernel_size\n",
    "        s = self.stride\n",
    "        (batch_count, input_channels) = (input.shape[0], input.shape[1])\n",
    "        output_channels = self.output_channels\n",
    "        (n, m) = (input.shape[2], input.shape[3])\n",
    "        (out_n, out_m) = ((n - k + 1 + s - 1) // s, (m - k + 1 + s - 1) // s)\n",
    "        out = np.zeros(shape = (batch_count, 1, input_channels*n*m))\n",
    "        for batch_id in range(batch_count):\n",
    "            for out_ch in range(self.output_channels):\n",
    "                for x in range(0, n - k + 1, s):\n",
    "                    for y in range(0, m - k + 1, s):\n",
    "                        for in_ch in range(input_channels):\n",
    "                            for nx in range(x,x+k):\n",
    "                                for ny in range(y,y+k):\n",
    "                                    dx, dy = nx-x, ny-y\n",
    "                                    id1 = out_ch*out_n*out_m + x*out_m + y\n",
    "                                    id2 = in_ch*n*m + nx*m + ny\n",
    "                                    out[batch_id, 0, id2] += self.kernel[dx, dy, in_ch, out_ch] * grad_loss_nxt[batch_id, 0, id1]\n",
    "        return out\n",
    "    def grad_kernel(self, input_data):\n",
    "        input = input_data.copy()\n",
    "        k = self.kernel_size\n",
    "        s = self.stride\n",
    "        (batch_count, input_channels) = (input.shape[0], input.shape[1])\n",
    "        output_channels = self.output_channels\n",
    "        (n, m) = (input.shape[2], input.shape[3])\n",
    "        (out_n, out_m) = ((n - k + 1 + s - 1) // s, (m - k + 1 + s - 1) // s)\n",
    "        out = np.zeros(shape = (batch_count, output_channels*out_n*out_m, k*k*input_channels*output_channels))\n",
    "        for batch_id in range(batch_count):\n",
    "            for out_ch in range(self.output_channels):\n",
    "                for x in range(0, n - k + 1, s):\n",
    "                    for y in range(0, m - k + 1, s):\n",
    "                        for in_ch in range(input_channels):\n",
    "                            for nx in range(x,x+k):\n",
    "                                for ny in range(y,y+k):\n",
    "                                    dx, dy = nx-x, ny-y\n",
    "                                    id1 = out_ch*out_n*out_m + x*out_m + y\n",
    "                                    id2 = dx*k*input_channels*output_channels + dy*input_channels*output_channels + in_ch*output_channels + out_ch\n",
    "                                    out[batch_id, id1, id2] += input[batch_id, in_ch, nx, ny]\n",
    "        return out\n",
    "    \n",
    "    def grad_bias(self, input_data):\n",
    "        input = input_data.copy()\n",
    "        k = self.kernel_size\n",
    "        s = self.stride\n",
    "        (batch_count, input_channels) = (input.shape[0], input.shape[1])\n",
    "        output_channels = self.output_channels\n",
    "        (n, m) = (input.shape[2], input.shape[3])\n",
    "        (out_n, out_m) = ((n - k + 1 + s - 1) // s, (m - k + 1 + s - 1) // s)\n",
    "        out = np.zeros(shape = (batch_count, output_channels*out_n*out_m, output_channels))\n",
    "        for batch_id in range(batch_count):\n",
    "            for out_ch in range(self.output_channels):\n",
    "                for x in range(0, n - k + 1, s):\n",
    "                    for y in range(0, m - k + 1, s):\n",
    "                        id1 = out_ch*out_n*out_m + x*out_m + y\n",
    "                        out[batch_id, id1, out_ch] += 1\n",
    "        return out\n",
    "        \n",
    "    def update_kernel(self, grad, learning_rate):\n",
    "        self.kernel -= learning_rate * np.mean(grad, axis=0).reshape(self.kernel.shape)\n",
    "    \n",
    "    def update_bias(self, grad,  learning_rate):\n",
    "        self.bias -= learning_rate * np.mean(grad, axis=0).reshape(self.bias.shape)\n",
    "        \n",
    "    def update_param(self, params_grad, learning_rate):\n",
    "        self.update_kernel(params_grad[0], learning_rate)\n",
    "        self.update_bias(params_grad[1], learning_rate)\n",
    "    \n",
    "    def grad_param(self, input_data):\n",
    "        return [self.grad_kernel(input_data), self.grad_bias(input_data)]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "iP5OYAooNKTO"
   },
   "source": [
    "Это была изначальная архитектура - так как для других переполнялась ОЗУ. До этого на работала примерно 35 минут за эпоху, после изменений стало 25 минут. Не сильное ускорение вышло - видимо потому, что я делаю вручную обновления градиента."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "w51XgzqFgy1I"
   },
   "outputs": [],
   "source": [
    "#(1,28,28) MaxPooling -> (1,14,14) Conv2dLayer -> (24, 12, 12) ReLU -> (24, 12, 12) MaxPooling -> (24, 6, 6) Flatten -> (864) DenseLayer -> (20) DenseLayer -> (10) -> Softmax -> (10) CrossEntropy -> (1)\n",
    "net = Network([MaxPooling(pool_size=(2,2), strides=2), Conv2DLayer(3,1,24), ReLU(), MaxPooling(pool_size=(2,2), strides=2), FlattenLayer(), DenseLayer(864, 20), DenseLayer(20, 10), Softmax()], loss=CrossEntropy())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "C2gzUPw-gw6F",
    "outputId": "a43140f9-3bd8-4e19-bc75-4d99ad40f16f"
   },
   "outputs": [
    {
     "metadata": {
      "tags": null
     },
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(60000, 1, 28, 28)\n"
     ]
    },
    {
     "metadata": {
      "tags": null
     },
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 468/468 [25:54<00:00,  3.32s/it]\n"
     ]
    },
    {
     "metadata": {
      "tags": null
     },
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      " 1 epoch: val 0.59 \n",
      "\n"
     ]
    },
    {
     "metadata": {
      "tags": null
     },
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 468/468 [25:54<00:00,  3.32s/it]\n"
     ]
    },
    {
     "metadata": {
      "tags": null
     },
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      " 2 epoch: val 0.70 \n",
      "\n"
     ]
    },
    {
     "metadata": {
      "tags": null
     },
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 468/468 [25:52<00:00,  3.32s/it]\n"
     ]
    },
    {
     "metadata": {
      "tags": null
     },
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      " 3 epoch: val 0.80 \n",
      "\n"
     ]
    },
    {
     "metadata": {
      "tags": null
     },
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 468/468 [25:52<00:00,  3.32s/it]\n"
     ]
    },
    {
     "metadata": {
      "tags": null
     },
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      " 4 epoch: val 0.84 \n",
      "\n"
     ]
    },
    {
     "metadata": {
      "tags": null
     },
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 468/468 [25:50<00:00,  3.31s/it]\n"
     ]
    },
    {
     "metadata": {
      "tags": null
     },
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      " 5 epoch: val 0.86 \n",
      "\n"
     ]
    },
    {
     "metadata": {
      "tags": null
     },
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 468/468 [25:46<00:00,  3.31s/it]\n"
     ]
    },
    {
     "metadata": {
      "tags": null
     },
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      " 6 epoch: val 0.87 \n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 468/468 [25:56<00:00,  3.32s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      " 7 epoch: val 0.88 \n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 468/468 [25:26<00:00,  3.26s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      " 8 epoch: val 0.88 \n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 468/468 [25:25<00:00,  3.26s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      " 9 epoch: val 0.88 \n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 468/468 [25:31<00:00,  3.27s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      " 10 epoch: val 0.88 \n",
      "\n"
     ]
    }
   ],
   "source": [
    "trainX = X_train.copy()\n",
    "print(trainX.shape)\n",
    "net.fit(trainX[::6], Y_train[::6], validation_split=0.25, \n",
    "            batch_size=16, nb_epoch=10, learning_rate=0.01)    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "RwoL0oT8NcQ3"
   },
   "source": [
    "Реализация выше доходила до 0.92 в лучшем случае, но в колабе я случайно перезапустил ячейку и результат не сохранился"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "SLbr4SC0HVVn"
   },
   "source": [
    "___"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "kf_WcrwvHWmU"
   },
   "source": [
    "То, что ниже - показать, что более полноценную сверточную сеть можно обучать без переполнения ОЗУ. Но я оставил на ночь Colab и он перестал работать.\n",
    "Как-то полноценно обучить не получилось из-за длительного времени работы моей реализации - Colab вылетал."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "id": "JL-6NUr_GbLm"
   },
   "outputs": [],
   "source": [
    "#(1,28,28) Conv2dLayer -> (24, 26, 26) ReLU -> (24, 26, 26) MaxPooling -> (24, 13, 13) Flatten -> (4056) DenseLayer -> (32) DenseLayer -> (10) -> Softmax -> (10) CrossEntropy -> (1)\n",
    "net_conv = Network([Conv2DLayer(3,1,24), ReLU(), MaxPooling(pool_size=(2,2), strides=2), FlattenLayer(), DenseLayer(4056, 32), DenseLayer(32, 10), Softmax()], loss=CrossEntropy())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "DXq793dQGjYE",
    "outputId": "5be7db3b-d702-49ae-b31a-83079dbb2777"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "  0%|          | 0/468 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(60000, 1, 28, 28)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 468/468 [1:29:12<00:00, 11.44s/it]\n",
      "  0%|          | 0/468 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      " 1 epoch: val 0.82 \n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 468/468 [1:31:53<00:00, 11.78s/it]\n",
      "  0%|          | 0/468 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      " 2 epoch: val 0.86 \n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 468/468 [1:31:22<00:00, 11.71s/it]\n",
      "  0%|          | 0/468 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      " 3 epoch: val 0.88 \n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 468/468 [1:31:30<00:00, 11.73s/it]\n",
      "  0%|          | 0/468 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      " 4 epoch: val 0.89 \n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 468/468 [1:31:12<00:00, 11.69s/it]\n",
      "  0%|          | 0/468 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      " 5 epoch: val 0.89 \n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 468/468 [1:26:32<00:00, 11.10s/it]\n",
      "  0%|          | 0/468 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      " 6 epoch: val 0.89 \n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 468/468 [1:23:23<00:00, 10.69s/it]\n",
      "  0%|          | 0/468 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      " 7 epoch: val 0.90 \n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 468/468 [1:24:27<00:00, 10.83s/it]\n",
      "  0%|          | 0/468 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      " 8 epoch: val 0.90 \n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 468/468 [1:25:57<00:00, 11.02s/it]\n",
      "  0%|          | 0/468 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      " 9 epoch: val 0.90 \n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 468/468 [1:24:35<00:00, 10.84s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      " 10 epoch: val 0.90 \n",
      "\n"
     ]
    }
   ],
   "source": [
    "trainX = X_train.copy()\n",
    "print(trainX.shape)\n",
    "net_conv.fit(trainX[::6], Y_train[::6], validation_split=0.25, \n",
    "            batch_size=16, nb_epoch=10, learning_rate=0.01)    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "#(1,28,28) Conv2dLayer -> (24, 26, 26) ReLU -> (24, 26, 26) MaxPooling -> (24, 13, 13) Flatten -> (4056) DenseLayer -> (32) DenseLayer -> (10) -> Softmax -> (10) CrossEntropy -> (1)\n",
    "net_conv = Network([Conv2DLayer(3,1,16), ReLU(), Conv2DLayer(3,16,16), ReLU(), Conv2DLayer(3,16,16), ReLU(), Conv2DLayer(3,16,16), ReLU(), Conv2DLayer(3,16,16), ReLU(), Conv2DLayer(3,16,16), ReLU(), MaxPooling(pool_size=(16,16), strides=1), FlattenLayer(), DenseLayer(16, 10), Softmax()], loss=CrossEntropy())\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "  0%|          | 0/468 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(60000, 1, 28, 28)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/468 [02:45<?, ?it/s]\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-12-fa81ec4cc3f0>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0mtrainX\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mX_train\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcopy\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtrainX\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 3\u001b[0;31m net_conv.fit(trainX[::6], Y_train[::6], validation_split=0.25, \n\u001b[0m\u001b[1;32m      4\u001b[0m             batch_size=16, nb_epoch=10, learning_rate=0.01)    \n",
      "\u001b[0;32m<ipython-input-2-6d5b3c028c5c>\u001b[0m in \u001b[0;36mfit\u001b[0;34m(self, trainX, trainY, validation_split, batch_size, nb_epoch, learning_rate)\u001b[0m\n\u001b[1;32m     67\u001b[0m                 \u001b[0mbatch_x\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtrain_x\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0mbatch_size\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m+\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0mbatch_size\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     68\u001b[0m                 \u001b[0mbatch_y\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtrain_y\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0mbatch_size\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m+\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0mbatch_size\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 69\u001b[0;31m                 \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtrain_step\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mbatch_x\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbatch_y\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlearning_rate\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     70\u001b[0m             \u001b[0;31m#validate\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     71\u001b[0m             \u001b[0mval_accuracy\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mevaluate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mval_x\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mval_y\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-2-6d5b3c028c5c>\u001b[0m in \u001b[0;36mtrain_step\u001b[0;34m(self, input_data, labels, learning_rate)\u001b[0m\n\u001b[1;32m     53\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     54\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mtrain_step\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minput_data\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlabels\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlearning_rate\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m0.001\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 55\u001b[0;31m         \u001b[0mgrad_list\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgrad_param\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minput_data\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlabels\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     56\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mupdate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mgrad_list\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlearning_rate\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     57\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-2-6d5b3c028c5c>\u001b[0m in \u001b[0;36mgrad_param\u001b[0;34m(self, input_data, labels)\u001b[0m\n\u001b[1;32m     29\u001b[0m         \u001b[0mcurrent_input\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0minput_data\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     30\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mlayer\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlayers\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 31\u001b[0;31m             \u001b[0mgradients_param\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlayer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbackward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcurrent_input\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     32\u001b[0m             \u001b[0mcurrent_input\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mlayer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcurrent_input\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     33\u001b[0m         \u001b[0mgradients_param\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mloss\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgrad_x\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcurrent_input\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlabels\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-1-2bb30ee95679>\u001b[0m in \u001b[0;36mbackward\u001b[0;34m(self, input_data)\u001b[0m\n\u001b[1;32m      5\u001b[0m         \u001b[0;32mpass\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      6\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mbackward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minput_data\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 7\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgrad_x\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minput_data\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgrad_param\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minput_data\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      8\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      9\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mgrad_x\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minput_data\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-5-136b406a0674>\u001b[0m in \u001b[0;36mgrad_x\u001b[0;34m(self, input_data)\u001b[0m\n\u001b[1;32m     62\u001b[0m                                     \u001b[0mid1\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mout_ch\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0mout_n\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0mout_m\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0mx\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0mout_m\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0my\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     63\u001b[0m                                     \u001b[0mid2\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0min_ch\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0mn\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0mm\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0mnx\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0mm\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0mny\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 64\u001b[0;31m                                     \u001b[0mout\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mbatch_id\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mid1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mid2\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m+=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mkernel\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mdx\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdy\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0min_ch\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mout_ch\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     65\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mout\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     66\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mgrad_kernel\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minput_data\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "trainX = X_train.copy()\n",
    "print(trainX.shape)\n",
    "net_conv.fit(trainX[::6], Y_train[::6], validation_split=0.25, \n",
    "            batch_size=16, nb_epoch=10, learning_rate=0.01)    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "colab": {
   "collapsed_sections": [],
   "name": "Practice02_akmal_ashirmatov.ipynb",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
