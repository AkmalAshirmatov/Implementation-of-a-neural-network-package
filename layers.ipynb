{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "X2GPoeVHUVTq"
   },
   "source": [
    "# Практическое задание 1\n",
    "\n",
    "\n",
    "\n",
    "## Замечания\n",
    "* Задание необходимо сдать боту до 15.11.2021\n",
    "* Соблюдаем кодекс чести (по нулям и списавшему, и давшему списать)\n",
    "* Можно (и нужно!) применять для реализации только библиотеку **Numpy**\n",
    "* Ничего, крому Numpy, нельзя использовать для реализации \n",
    "* **Keras** используется только для тестирования Вашей реализации\n",
    "* Если какой-то из классов не проходит приведенные тесты, то соответствующее задание не оценивается\n",
    "* Возможно использование дополнительных (приватных) тестов\n",
    " "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "id": "hT1dNA7Gb35a"
   },
   "outputs": [],
   "source": [
    "# Вам понадобится для реализации\n",
    "import numpy as np\n",
    "# Нужно для тестирования\n",
    "from tensorflow import keras\n",
    "import keras.layers as layers"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "0G7sQn2ZXh9Y"
   },
   "source": [
    "* Вспомогательные функции для тестирования"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "id": "_vXhfmihINmY"
   },
   "outputs": [],
   "source": [
    "def compare_tensors(x, y, tol=0.001, test_name='Test'):\n",
    "  assert (x.shape == y.shape), test_name + ' different shapes'\n",
    "  diff = np.sum((y - x)**2)\n",
    "  assert (diff < tol), test_name + ' Failed!'\n",
    "  print (test_name + ' Passed!')\n",
    "  return"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "id": "Pq_lVMUle_ii"
   },
   "outputs": [],
   "source": [
    "def compare_tensors_array(x, y, tol=0.001, test_name='Test'):\n",
    "  assert (len(x) == len(y)), test_name + ' different lengths'\n",
    "  for i in range(len(x)):\n",
    "    t = test_name + ' subtest ' + str(i)\n",
    "    compare_tensors(x[i], y[i], tol=tol, test_name=t)\n",
    "  print (test_name + ' Passed!')\n",
    "  return"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "a4iNYGlDyNAF"
   },
   "source": [
    "* Шаблон класса любой операции (слоя), которую Вам необходимо будет реализовать"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "id": "05lYhmjMSm0s"
   },
   "outputs": [],
   "source": [
    "class Layer(object):\n",
    "    def __init__(self):\n",
    "        self.name = 'Layer'       \n",
    "    def forward(self, input_data):\n",
    "        pass"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "caE-Xn1ZY79p"
   },
   "source": [
    "\n",
    "\n",
    "---\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "R9yuQiPjyOBZ"
   },
   "source": [
    "* (1 балл) Реализация \"спрямляющего\" слоя Flatten"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {
    "id": "zJIqDFDC-8Gh"
   },
   "outputs": [],
   "source": [
    "class FlattenLayer(Layer):\n",
    "    def __init__(self):\n",
    "      self.name = 'Flatten'\n",
    "    def forward(self, input_data):\n",
    "      # На входе - четырехмерный тензор вида [batch, input_channels, height, width]\n",
    "      # Преобразуем в двухмерный тензор: при этом по первой размерности НЕ преобразуем\n",
    "      # Выкладываем данные: сначала по последней размерности, затем по предпоследней и т.д.\n",
    "      # Нужно заполнить Numpy-тензор out \n",
    "      length = input_data.shape[1] * input_data.shape[2] \\\n",
    "            * input_data.shape[3]\n",
    "      out = np.random.randn(input_data.shape[0], length)\n",
    "      for batch_id in range(input_data.shape[0]):\n",
    "          id = 0\n",
    "          for j in range(input_data.shape[2]):\n",
    "              for k in range(input_data.shape[3]):\n",
    "                  for i in range(input_data.shape[1]):\n",
    "                      out[batch_id, id] = input_data[batch_id, i, j, k]\n",
    "                      id += 1\n",
    "      return out"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "vAD92fJkYcNI"
   },
   "source": [
    "* Функция предварительного тестирования слоя **Flatten**\n",
    "* Функции с названием \"**test_**\" не менять\n",
    "* Вы можете самостоятельно поиграться с параметрами типа B/C/H/W etc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {
    "id": "eZsoCXd1HioL"
   },
   "outputs": [],
   "source": [
    "def test_FlattenLayer():\n",
    "  B = 1\n",
    "  C = 1\n",
    "  H = 3\n",
    "  W = 3\n",
    "  x = np.random.randn(B, C, H, W)\n",
    "  y = layers.Flatten(data_format='channels_first')\n",
    "  y_keras = y(x).numpy()\n",
    "  y_out = FlattenLayer().forward(x)\n",
    "  compare_tensors(y_keras, y_out, tol=0.001, test_name='Test Flatten 1')\n",
    "  B = 1\n",
    "  C = 2\n",
    "  H = 3\n",
    "  W = 3\n",
    "  x = np.random.randn(B, C, H, W)\n",
    "  y = layers.Flatten(data_format='channels_first')\n",
    "  y_keras = y(x).numpy()\n",
    "  y_out = FlattenLayer().forward(x)\n",
    "  compare_tensors(y_keras, y_out, tol=0.001, test_name='Test Flatten 2')\n",
    "  return"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "eMe7eba6Yu4a"
   },
   "source": [
    "* Запуск теста слоя Flatten\n",
    "* Нужно, чтобы все тесты были '*Passed!*'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "pST9EihGKTEh",
    "outputId": "4a6ad8d4-63f9-4daf-b7c6-9249592aad47"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test Flatten 1 Passed!\n",
      "Test Flatten 2 Passed!\n"
     ]
    }
   ],
   "source": [
    "test_FlattenLayer()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "7IsZsCozs2jc"
   },
   "outputs": [],
   "source": [
    "def my_test_FlattenLayer():\n",
    "  for iter in range(100):\n",
    "    B = np.random.randint(1,21)\n",
    "    C = np.random.randint(1,21)\n",
    "    H = np.random.randint(1,21)\n",
    "    W = np.random.randint(1,21)\n",
    "    x = np.random.randn(B, C, H, W)\n",
    "    y = layers.Flatten(data_format='channels_first')\n",
    "    y_keras = y(x).numpy()\n",
    "    y_out = FlattenLayer().forward(x)\n",
    "    compare_tensors(y_keras, y_out, tol=0.001, test_name='Test Flatten 1')\n",
    "  return"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "ZK9REvLLs4B0"
   },
   "outputs": [],
   "source": [
    "my_test_FlattenLayer()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "hOngWlbqyQJ9"
   },
   "source": [
    "\n",
    "\n",
    "---\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "q6NfaNFKZGOk"
   },
   "source": [
    "* (1 балл) Реализация слоя субдискретизации **Global Average Pooling**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {
    "id": "7_XMSI137G04"
   },
   "outputs": [],
   "source": [
    "class GAP2DLayer(Layer):\n",
    "    def __init__(self):\n",
    "      self.name = 'GAP2D'\n",
    "    def forward(self, input_data):\n",
    "      # На входе - четырехмерный тензор вида [batch, input_channels, height, width]\n",
    "      # Сворачиваем по двум последним размерностям (то есть на выходе - минус две размерности)\n",
    "      # Нужно заполнить Numpy-тензор out \n",
    "      out = np.zeros([input_data.shape[0], input_data.shape[1]])\n",
    "      for batch_id in range(input_data.shape[0]):\n",
    "          for input_channels_id in range(input_data.shape[1]):\n",
    "              array = input_data[batch_id, input_channels_id, :, :]\n",
    "              out[batch_id, input_channels_id] = np.mean(array)\n",
    "      return out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {
    "id": "u9KLCdrTLT-j"
   },
   "outputs": [],
   "source": [
    "def test_GAP2DLayer():\n",
    "  B = 1\n",
    "  C = 1\n",
    "  H = 3\n",
    "  W = 3\n",
    "  x = np.random.randn(B, C, H, W)\n",
    "  y = layers.GlobalAveragePooling2D(data_format='channels_first')\n",
    "  y_keras = y(x).numpy()\n",
    "  y_out = GAP2DLayer().forward(x)\n",
    "  compare_tensors(y_keras, y_out, tol=0.001, test_name='Test GAP2D 1')\n",
    "  B = 1\n",
    "  C = 2\n",
    "  H = 3\n",
    "  W = 3\n",
    "  x = np.random.randn(B, C, H, W)\n",
    "  y = layers.GlobalAveragePooling2D(data_format='channels_first')\n",
    "  y_keras = y(x).numpy()\n",
    "  y_out = GAP2DLayer().forward(x)\n",
    "  compare_tensors(y_keras, y_out, tol=0.001, test_name='Test GAP2D 2')\n",
    "  return"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "AbuDxhloPLKs",
    "outputId": "f41a9f1d-8b3c-4697-f80d-c020bbaabdd9"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test GAP2D 1 Passed!\n",
      "Test GAP2D 2 Passed!\n"
     ]
    }
   ],
   "source": [
    "test_GAP2DLayer()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "RUWVVQ3Ovpuw"
   },
   "outputs": [],
   "source": [
    "def my_test_GAP2DLayer():\n",
    "  for iter in range(20):\n",
    "    B = np.random.randint(1,21)\n",
    "    C = np.random.randint(1,21)\n",
    "    H = np.random.randint(1,21)\n",
    "    W = np.random.randint(1,21)\n",
    "    x = np.random.randn(B, C, H, W)\n",
    "    y = layers.GlobalAveragePooling2D(data_format='channels_first')\n",
    "    y_keras = y(x).numpy()\n",
    "    y_out = GAP2DLayer().forward(x)\n",
    "    compare_tensors(y_keras, y_out, tol=0.001, test_name='Test GAP2D 1')\n",
    "  return"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "NlrWLahxvQMe"
   },
   "outputs": [],
   "source": [
    "my_test_GAP2DLayer()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Nbse3gb2ySI_"
   },
   "source": [
    "\n",
    "\n",
    "---\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "l2CCmuiZZTXp"
   },
   "source": [
    "* (2 балла) Реализация слоя субдискретизации **MaxPooling**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {
    "id": "fFSW6Xpp-8NS"
   },
   "outputs": [],
   "source": [
    "class MaxPool2DLayer(Layer):\n",
    "    def __init__(self, pool_size=2, stride=2):\n",
    "      self.name = 'MaxPool2D'\n",
    "      self.pool_size = pool_size\n",
    "      self.stride = stride\n",
    "    def forward(self, input_data):\n",
    "      # На входе - четырехмерный тензор вида [batch, input_channels, height, width]\n",
    "      # Нужно заполнить Numpy-тензор out \n",
    "      input = input_data.copy()\n",
    "      k = self.pool_size\n",
    "      s = self.stride\n",
    "      (batch_count, channels_count) = (input.shape[0], input.shape[1])\n",
    "      (n, m) = (input.shape[2], input.shape[3])\n",
    "      (out_n, out_m) = ((n - k + 1 + s - 1) // s, (m - k + 1 + s - 1) // s)\n",
    "      assert out_n >= 0\n",
    "      assert out_m >= 0\n",
    "      out = np.zeros([batch_count, channels_count, out_n, out_m])\n",
    "      for batch_id in range(batch_count):\n",
    "          for ch_id in range(channels_count):\n",
    "              for x in range(0, n - k + 1, s):\n",
    "                  for y in range(0, m - k + 1, s):\n",
    "                      a = input[batch_id, ch_id, x:x + k, y:y + k]\n",
    "                      out[batch_id, ch_id, x // s, y // s] = np.amax(a)\n",
    "      return out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {
    "id": "kvCIn_aXUPkD"
   },
   "outputs": [],
   "source": [
    "def test_MaxPool2DLayer():\n",
    "  B = 1\n",
    "  C = 1\n",
    "  H = 4\n",
    "  W = 4\n",
    "  pool_size = 2\n",
    "  stride = 2\n",
    "  x = np.random.randn(B, C, H, W)\n",
    "  y = layers.MaxPooling2D(pool_size=pool_size, strides=stride, padding=\"valid\", data_format='channels_first')\n",
    "  y_keras = y(x).numpy()\n",
    "  y_out = MaxPool2DLayer(pool_size=pool_size, stride=stride).forward(x)\n",
    "  compare_tensors(y_keras, y_out, tol=0.001, test_name='Test MaxPool2D 1')\n",
    "  B = 2\n",
    "  C = 2\n",
    "  H = 3\n",
    "  W = 3\n",
    "  pool_size = 2\n",
    "  stride = 1  \n",
    "  x = np.random.randn(B, C, H, W)\n",
    "  y = layers.MaxPooling2D(pool_size=pool_size, strides=stride, padding=\"valid\", data_format='channels_first')\n",
    "  y_keras = y(x).numpy()\n",
    "  y_out = MaxPool2DLayer(pool_size=pool_size, stride=stride).forward(x)\n",
    "  compare_tensors(y_keras, y_out, tol=0.001, test_name='Test MaxPool2D 2')\n",
    "  return"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "GHRtvEIpVsYe",
    "outputId": "efddadd0-52d1-4e00-b938-215fd360837f"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test MaxPool2D 1 Passed!\n",
      "Test MaxPool2D 2 Passed!\n"
     ]
    }
   ],
   "source": [
    "test_MaxPool2DLayer()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "3U81sYKroVwV"
   },
   "outputs": [],
   "source": [
    "def my_test_MaxPool2DLayer():\n",
    "  for iter in range(50):  \n",
    "      B = 10\n",
    "      C = np.random.randint(1,6)\n",
    "      H = np.random.randint(1,21)\n",
    "      W = np.random.randint(1,21)\n",
    "      pool_size = np.random.randint(1,6)\n",
    "      stride = np.random.randint(1,6)\n",
    "      if (H-pool_size+1 <= 0 or W-pool_size+1 <= 0):\n",
    "        continue\n",
    "      x = np.random.randn(B, C, H, W)\n",
    "      y = layers.MaxPooling2D(pool_size=pool_size, strides=stride, padding=\"valid\", data_format='channels_first')\n",
    "      y_keras = y(x).numpy()\n",
    "      y_out = MaxPool2DLayer(pool_size=pool_size, stride=stride).forward(x)\n",
    "      compare_tensors(y_keras, y_out, tol=0.001, test_name='Test MaxPool2D 1')\n",
    "  return"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "_BTWUMf89z6A"
   },
   "outputs": [],
   "source": [
    "my_test_MaxPool2DLayer()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "tLtOD86byTQ-"
   },
   "source": [
    "\n",
    "\n",
    "---\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "zruUuHDeZf-4"
   },
   "source": [
    "* (3 балла) Реализация слоя **активации** (поддерживаются **relu**, **sigmoid**, **softmax**)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "yFytq6FOByQ9"
   },
   "outputs": [],
   "source": [
    "class ActivationLayer(Layer):\n",
    "    def __init__(self, activation='relu'):\n",
    "      # Активация (поддерживаем 'relu', 'sigmoid', 'softmax')\n",
    "      self.name = 'Activation'\n",
    "      self.activation = activation\n",
    "    def forward(self, input_data):   \n",
    "      # На входе:\n",
    "      # четырехмерный тензор вида [batch, input_channels, height, width] для 'relu', 'sigmoid'\n",
    "      # или двухмерный тензор вида [batch, logits]\n",
    "      # SoftMax применяется по последней размерности\n",
    "      # Нужно заполнить Numpy-тензор out \n",
    "      def f(x):\n",
    "          if self.activation == 'relu':\n",
    "              return max(0, x)\n",
    "          if self.activation == 'sigmoid':\n",
    "              return 1 / (1 + np.exp(-x))\n",
    "          if self.activation == 'softmax':\n",
    "              return np.exp(x)\n",
    "\n",
    "      def g(array):\n",
    "          for (index, x) in np.ndenumerate(array):\n",
    "              array[index] = f(x)\n",
    "          array /= np.sum(array)\n",
    "          return array\n",
    "\n",
    "      out = input_data.copy()\n",
    "      if self.activation == 'relu' or self.activation == 'sigmoid':\n",
    "          for (index, x) in np.ndenumerate(out):\n",
    "              out[index] = f(x)\n",
    "          return out\n",
    "      return np.apply_along_axis(g, -1, out)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "-RnOBuLTWcIf"
   },
   "outputs": [],
   "source": [
    "def test_ActivationLayer():\n",
    "  B = 1\n",
    "  C = 1\n",
    "  H = 4\n",
    "  W = 4\n",
    "  activation = 'relu'\n",
    "  x = np.random.randn(B, C, H, W)\n",
    "  y = layers.Activation(activation)\n",
    "  y_keras = y(x).numpy()\n",
    "  y_out = ActivationLayer(activation=activation).forward(x)\n",
    "  compare_tensors(y_keras, y_out, tol=0.001, test_name='Test Activation 1')\n",
    "  B = 2\n",
    "  C = 2\n",
    "  H = 3\n",
    "  W = 3\n",
    "  activation = 'sigmoid'  \n",
    "  x = np.random.randn(B, C, H, W)\n",
    "  y = layers.Activation(activation)\n",
    "  y_keras = y(x).numpy()\n",
    "  y_out = ActivationLayer(activation=activation).forward(x)\n",
    "  compare_tensors(y_keras, y_out, tol=0.001, test_name='Test Activation 2')\n",
    "  B = 3\n",
    "  C = 10\n",
    "  activation = 'softmax'\n",
    "  x = np.random.randn(B, C)\n",
    "  y = layers.Activation(activation)\n",
    "  y_keras = y(x).numpy()\n",
    "  y_out = ActivationLayer(activation=activation).forward(x)\n",
    "  compare_tensors(y_keras, y_out, tol=0.001, test_name='Test Activation 3')  \n",
    "  return"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "n3Bwcpw7X4_m",
    "outputId": "079635ea-8f95-4d17-9c8d-73e4c0ae530a"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test Activation 1 Passed!\n",
      "Test Activation 2 Passed!\n",
      "Test Activation 3 Passed!\n"
     ]
    }
   ],
   "source": [
    "test_ActivationLayer()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "RBtj_O2Mx_3i"
   },
   "outputs": [],
   "source": [
    "def my_test_ActivationLayer():\n",
    "  for iter in range(30):  \n",
    "    B = np.random.randint(1,21)\n",
    "    C = np.random.randint(1,21)\n",
    "    H = np.random.randint(1,21)\n",
    "    W = np.random.randint(1,21)\n",
    "    activation = 'relu'\n",
    "    x = np.random.randn(B, C, H, W)\n",
    "    y = layers.Activation(activation)\n",
    "    y_keras = y(x).numpy()\n",
    "    y_out = ActivationLayer(activation=activation).forward(x)\n",
    "    compare_tensors(y_keras, y_out, tol=0.001, test_name='Test Activation 1')\n",
    "  for iter in range(20):  \n",
    "    B = np.random.randint(1,21)\n",
    "    C = np.random.randint(1,21)\n",
    "    H = np.random.randint(1,21)\n",
    "    W = np.random.randint(1,21)\n",
    "    activation = 'sigmoid'  \n",
    "    x = np.random.randn(B, C, H, W)\n",
    "    y = layers.Activation(activation)\n",
    "    y_keras = y(x).numpy()\n",
    "    y_out = ActivationLayer(activation=activation).forward(x)\n",
    "    compare_tensors(y_keras, y_out, tol=0.001, test_name='Test Activation 2')\n",
    "  for iter in range(20):  \n",
    "    B = np.random.randint(1,21)\n",
    "    C = np.random.randint(1,21)\n",
    "    activation = 'softmax'\n",
    "    x = np.random.randn(B, C)\n",
    "    y = layers.Activation(activation)\n",
    "    y_keras = y(x).numpy()\n",
    "    y_out = ActivationLayer(activation=activation).forward(x)\n",
    "    compare_tensors(y_keras, y_out, tol=0.001, test_name='Test Activation 3')  \n",
    "  return"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "5WdVsXVYyBdT"
   },
   "outputs": [],
   "source": [
    "my_test_ActivationLayer()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "5YtW9jiayUdV"
   },
   "source": [
    "\n",
    "\n",
    "---\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "7dOJl776Z0t7"
   },
   "source": [
    "* (3 балла) Реализация слоя пакетной нормализации **BatchNorm** (как для режима train, так и для режима test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "993xMUgKDxZK"
   },
   "outputs": [],
   "source": [
    "# Hint\n",
    "# Train mode:\n",
    "# out = (batch - mean(batch)) / sqrt(var(batch) + epsilon) * gamma + beta\n",
    "# moving_mean = moving_mean * momentum + mean(batch) * (1 - momentum)\n",
    "# moving_var = moving_var * momentum + var(batch) * (1 - momentum)\n",
    "# Test mode:\n",
    "# (batch - moving_mean) / sqrt(moving_var + epsilon) * gamma + beta\n",
    "\n",
    "class BatchNormLayer(Layer):\n",
    "    def __init__(self, momentum=0.99, epsilon=0.001, beta_init=None, gamma_init=None,\n",
    "                 moving_mean_init=None, moving_var_init=None,\n",
    "                 mode='train', input_channels=2):\n",
    "      # mode: 'train', 'test'\n",
    "      # Параметры gamma, beta, mean, var - все имеют размерность по количеству карт input_channels   \n",
    "      self.name = 'BatchNorm'\n",
    "      self.momentum = momentum\n",
    "      self.epsilon = epsilon\n",
    "      self.beta = beta_init\n",
    "      self.gamma = gamma_init\n",
    "      self.moving_mean = moving_mean_init\n",
    "      self.moving_var = moving_var_init\n",
    "      self.mode = mode\n",
    "      self.input_channels = input_channels\n",
    "    def forward(self, input_data):   \n",
    "      # На входе - четырехмерный тензор вида [batch, input_channels, height, width]\n",
    "      # 1) Нужно заполнить Numpy-тензор out (той же размерности, что и вход)\n",
    "      # 2) Нужно обновить moving_mean и moving_var в режиме 'train'\n",
    "      assert input_data.shape[1] == self.input_channels\n",
    "      out = input_data.copy()\n",
    "      momentum = self.momentum\n",
    "      for ch_id in range(self.input_channels):\n",
    "          if self.mode == 'train':\n",
    "              my_card = input_data[:, ch_id, :, :].copy()\n",
    "              mean = np.mean(my_card)\n",
    "              my_card -= mean\n",
    "              my_card = my_card ** 2\n",
    "              var = np.sum(my_card) / np.prod(my_card.shape)\n",
    "              moving_mean = self.moving_mean[ch_id]\n",
    "              moving_var = self.moving_var[ch_id]\n",
    "              moving_mean = moving_mean * momentum + mean * (1 - momentum)\n",
    "              moving_var = moving_var * momentum + var * (1 - momentum)\n",
    "              self.moving_mean[ch_id] = moving_mean\n",
    "              self.moving_var[ch_id] = moving_var\n",
    "          else:\n",
    "              mean = self.moving_mean[ch_id]\n",
    "              var = self.moving_var[ch_id]\n",
    "          my_card = input_data[:, ch_id, :, :].copy()\n",
    "          my_card -= mean\n",
    "          my_card /= np.sqrt(var) + self.epsilon\n",
    "          my_card *= self.gamma[ch_id]\n",
    "          my_card += self.beta[ch_id]\n",
    "          out[:, ch_id, :, :] = my_card\n",
    "      return out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "NhZ6TK-RYfm-"
   },
   "outputs": [],
   "source": [
    "def test_BatchNormLayer():\n",
    "  B = 2\n",
    "  C = 2\n",
    "  H = 4\n",
    "  W = 4\n",
    "  beta_init = 0 * np.ones(C)\n",
    "  gamma_init = 1 * np.ones(C)\n",
    "  moving_mean_init = 0 * np.ones(C)\n",
    "  moving_var_init= 1 * np.ones(C)\n",
    "  momentum = 0.99\n",
    "  epsilon = 0.001\n",
    "  mode = 'train'\n",
    "  x = np.random.randn(B, C, H, W)\n",
    "  y = layers.BatchNormalization(axis=1, momentum=momentum, epsilon=epsilon, trainable=True)\n",
    "  y_keras = y(x, training=True).numpy()\n",
    "  y.set_weights([gamma_init, beta_init, moving_mean_init, moving_var_init])\n",
    "  y_keras = y(x, training=True).numpy()\n",
    "  y_out_layer = BatchNormLayer(momentum=momentum, epsilon=epsilon, beta_init=beta_init, gamma_init=gamma_init,\n",
    "                 moving_mean_init=moving_mean_init, moving_var_init=moving_var_init,\n",
    "                 mode=mode, input_channels=C)\n",
    "  y_out = y_out_layer.forward(x)\n",
    "  compare_tensors(y_keras, y_out, tol=0.001, test_name='Test BatchNorm 1')\n",
    "  compare_tensors_array(y.get_weights(), \n",
    "                        [y_out_layer.gamma, y_out_layer.beta, y_out_layer.moving_mean, y_out_layer.moving_var],\n",
    "                        tol=0.00001, test_name='Test BatchNorm 1.1')\n",
    "  B = 2 \n",
    "  C = 2 \n",
    "  H = 4 \n",
    "  W = 4 \n",
    "  beta_init = 1 * np.ones(C)\n",
    "  gamma_init = 0 * np.ones(C)\n",
    "  moving_mean = 0 * np.ones(C)\n",
    "  moving_var = 1 * np.ones(C)\n",
    "  momentum = 0.99\n",
    "  epsilon = 0.001\n",
    "  mode = 'test'\n",
    "  x = np.random.randn(B, C, H, W)\n",
    "  y = layers.BatchNormalization(axis=1, momentum=momentum, epsilon=epsilon, trainable=False)\n",
    "  y_keras = y(x, training=False).numpy()\n",
    "  y.set_weights([gamma_init, beta_init, moving_mean_init, moving_var_init])\n",
    "  y_keras = y(x, training=False).numpy()\n",
    "  y_out_layer = BatchNormLayer(momentum=momentum, epsilon=epsilon, beta_init=beta_init, gamma_init=gamma_init,\n",
    "                 moving_mean_init=moving_mean_init, moving_var_init=moving_var_init,\n",
    "                 mode=mode, input_channels=C)\n",
    "  y_out = y_out_layer.forward(x)\n",
    "  compare_tensors(y_keras, y_out, tol=0.001, test_name='Test BatchNorm 2')  \n",
    "  compare_tensors_array(y.get_weights(), \n",
    "                        [y_out_layer.gamma, y_out_layer.beta, y_out_layer.moving_mean, y_out_layer.moving_var],\n",
    "                        tol=0.00001, test_name='Test BatchNorm 2.1')\n",
    "  return"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "kZ2F0Xg1Yf94",
    "outputId": "5e6e3e57-293e-4859-bffd-7d5f7845499b"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test BatchNorm 1 Passed!\n",
      "Test BatchNorm 1.1 subtest 0 Passed!\n",
      "Test BatchNorm 1.1 subtest 1 Passed!\n",
      "Test BatchNorm 1.1 subtest 2 Passed!\n",
      "Test BatchNorm 1.1 subtest 3 Passed!\n",
      "Test BatchNorm 1.1 Passed!\n",
      "Test BatchNorm 2 Passed!\n",
      "Test BatchNorm 2.1 subtest 0 Passed!\n",
      "Test BatchNorm 2.1 subtest 1 Passed!\n",
      "Test BatchNorm 2.1 subtest 2 Passed!\n",
      "Test BatchNorm 2.1 subtest 3 Passed!\n",
      "Test BatchNorm 2.1 Passed!\n"
     ]
    }
   ],
   "source": [
    "test_BatchNormLayer()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "GKD_mRqKGTSJ"
   },
   "outputs": [],
   "source": [
    "def my_test_BatchNormLayer():\n",
    "  for iter in range(30):\n",
    "      B = np.random.randint(1,6)\n",
    "      C = np.random.randint(1,6)\n",
    "      H = np.random.randint(1,11)\n",
    "      W = np.random.randint(1,11)\n",
    "      beta_init = np.random.randn() * np.ones(C)\n",
    "      gamma_init = np.random.randn() * np.ones(C)\n",
    "      moving_mean_init = 0 * np.ones(C)\n",
    "      moving_var_init = 1 * np.ones(C)\n",
    "      momentum = 0.99\n",
    "      epsilon = 0.001\n",
    "      mode = 'train'\n",
    "      x = np.random.randn(B, C, H, W)\n",
    "      y = layers.BatchNormalization(axis=1, momentum=momentum, epsilon=epsilon, trainable=True)\n",
    "      y_keras = y(x, training=True).numpy()\n",
    "      y.set_weights([gamma_init, beta_init, moving_mean_init, moving_var_init])\n",
    "      y_keras = y(x, training=True).numpy()\n",
    "      y_out_layer = BatchNormLayer(momentum=momentum, epsilon=epsilon, beta_init=beta_init, gamma_init=gamma_init,\n",
    "                    moving_mean_init=moving_mean_init, moving_var_init=moving_var_init,\n",
    "                    mode=mode, input_channels=C)\n",
    "      y_out = y_out_layer.forward(x)\n",
    "      compare_tensors(y_keras, y_out, tol=0.001, test_name='Test BatchNorm 1')\n",
    "      compare_tensors_array(y.get_weights(), \n",
    "                            [y_out_layer.gamma, y_out_layer.beta, y_out_layer.moving_mean, y_out_layer.moving_var],\n",
    "                            tol=0.0001, test_name='Test BatchNorm 1.1')\n",
    "      \n",
    "      B = np.random.randint(1,6)\n",
    "      H = np.random.randint(1,11)\n",
    "      W = np.random.randint(1,11)\n",
    "      #C don't change\n",
    "      beta_init = np.random.randn() * np.ones(C)\n",
    "      gamma_init = np.random.randn() * np.ones(C)\n",
    "      momentum = 0.99\n",
    "      epsilon = 0.001\n",
    "      mode = 'test'\n",
    "      x = np.random.randn(B, C, H, W)\n",
    "      y = layers.BatchNormalization(axis=1, momentum=momentum, epsilon=epsilon, trainable=False)\n",
    "      y_keras = y(x, training=False).numpy()\n",
    "      y.set_weights([gamma_init, beta_init, moving_mean_init, moving_var_init])\n",
    "      y_keras = y(x, training=False).numpy()\n",
    "      y_out_layer = BatchNormLayer(momentum=momentum, epsilon=epsilon, beta_init=beta_init, gamma_init=gamma_init,\n",
    "                    moving_mean_init=moving_mean_init, moving_var_init=moving_var_init,\n",
    "                    mode=mode, input_channels=C)\n",
    "      y_out = y_out_layer.forward(x)\n",
    "      compare_tensors(y_keras, y_out, tol=0.001, test_name='Test BatchNorm 2')  \n",
    "      compare_tensors_array(y.get_weights(), \n",
    "                            [y_out_layer.gamma, y_out_layer.beta, y_out_layer.moving_mean, y_out_layer.moving_var],\n",
    "                            tol=0.0001, test_name='Test BatchNorm 2.1')\n",
    "  return"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "_I3SdM7_GVS4"
   },
   "outputs": [],
   "source": [
    "my_test_BatchNormLayer()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "fJKt_4mCyV4r"
   },
   "source": [
    "\n",
    "\n",
    "---\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "-f_m9DaTaHio"
   },
   "source": [
    "* (1 балл) Реализация **полносвязного** слоя"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "1Ln22ERp8mC5"
   },
   "outputs": [],
   "source": [
    "class DenseLayer(Layer):\n",
    "    def __init__(self, input_dim, output_dim, W_init=None, b_init=None):\n",
    "      self.name = 'Dense'\n",
    "      self.input_dim = input_dim\n",
    "      self.output_dim = output_dim\n",
    "      self.W = W_init\n",
    "      self.b = b_init\n",
    "    def forward(self, input_data):\n",
    "      # На входе - двухмерный тензор вида [batch, input_channels]\n",
    "      # Работаем по второй размерности, по первой размерности НЕ преобразуем\n",
    "      # Вначале нужно проверить на согласование размерностей входных данных и ядра!\n",
    "      assert input_data.shape[1] == self.W.shape[0]\n",
    "      out = np.matmul(input_data, self.W) + self.b\n",
    "      return out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "Ln9JIKL8YhZF"
   },
   "outputs": [],
   "source": [
    "def test_DenseLayer():\n",
    "  B = 1\n",
    "  C_IN = 10\n",
    "  C_OUT = 5\n",
    "  x = np.random.randn(B, C_IN)\n",
    "  W_init = np.random.randn(C_IN, C_OUT)\n",
    "  b_init = np.random.randn(C_OUT)\n",
    "  y = layers.Dense(C_OUT, use_bias=True)\n",
    "  y_keras = y(x).numpy()\n",
    "  y.set_weights([W_init, b_init])\n",
    "  y_keras = y(x).numpy()\n",
    "  y_out = DenseLayer(C_IN, C_OUT, W_init=W_init, b_init=b_init).forward(x)\n",
    "  compare_tensors(y_keras, y_out, tol=0.001, test_name='Test Dense 1')\n",
    "  B = 2\n",
    "  C_IN = 5\n",
    "  C_OUT = 10\n",
    "  x = np.random.randn(B, C_IN)\n",
    "  W_init = np.random.randn(C_IN, C_OUT)\n",
    "  b_init = np.random.randn(C_OUT)\n",
    "  y = layers.Dense(C_OUT, use_bias=True, input_shape=(C_IN,))\n",
    "  y_keras = y(x).numpy()\n",
    "  y.set_weights([W_init, b_init])\n",
    "  y_keras = y(x).numpy()\n",
    "  y_out = DenseLayer(C_IN, C_OUT, W_init=W_init, b_init=b_init).forward(x)\n",
    "  compare_tensors(y_keras, y_out, tol=0.001, test_name='Test Dense 2')\n",
    "  return"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "4NA_KNjZYhec",
    "outputId": "ea8b9e1b-0328-4548-926a-f398d51703c0"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test Dense 1 Passed!\n",
      "Test Dense 2 Passed!\n"
     ]
    }
   ],
   "source": [
    "test_DenseLayer()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "PerXORFcx1cI"
   },
   "outputs": [],
   "source": [
    "def my_test_DenseLayer():\n",
    "  for iter in range(20):\n",
    "    B = np.random.randint(1,21)\n",
    "    C_IN = np.random.randint(1,21)\n",
    "    C_OUT = np.random.randint(1,21)\n",
    "    x = np.random.randn(B, C_IN)\n",
    "    W_init = np.random.randn(C_IN, C_OUT)\n",
    "    b_init = np.random.randn(C_OUT)\n",
    "    y = layers.Dense(C_OUT, use_bias=True)\n",
    "    y_keras = y(x).numpy()\n",
    "    y.set_weights([W_init, b_init])\n",
    "    y_keras = y(x).numpy()\n",
    "    y_out = DenseLayer(C_IN, C_OUT, W_init=W_init, b_init=b_init).forward(x)\n",
    "    compare_tensors(y_keras, y_out, tol=0.001, test_name='Test Dense 1')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "yaHktWbFx3BG"
   },
   "outputs": [],
   "source": [
    "my_test_DenseLayer()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "966wyQwryXun"
   },
   "source": [
    "\n",
    "\n",
    "---\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "j6A8OLysaS3Q"
   },
   "source": [
    "* (2 балла) Реализация **сверточного** слоя"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "WFi_DV-DL9nd"
   },
   "outputs": [],
   "source": [
    "class Conv2DLayer(Layer):\n",
    "    def __init__(self, kernel_size=3, input_channels=2, output_channels=3, \n",
    "                 padding='same', stride=1, K_init=None, b_init=None):\n",
    "      # padding: 'same' или 'valid'\n",
    "      # Работаем с квадратными ядрами, поэтому kernel_size - одно число\n",
    "      # Работаем с единообразным сдвигом, поэтому stride - одно число\n",
    "      # Фильтр размерности [kernel_size, kernel_size, input_channels, output_channels]\n",
    "      self.name = 'Conv2D'\n",
    "      self.kernel_size = kernel_size\n",
    "      self.input_channels = input_channels\n",
    "      self.output_channels = output_channels\n",
    "      self.kernel = K_init\n",
    "      self.bias = b_init\n",
    "      self.padding = padding\n",
    "      self.stride = stride\n",
    "    def forward(self, input_data):\n",
    "      # На входе - четырехмерный тензор вида [batch, input_channels, height, width]\n",
    "      # Вначале нужно проверить на согласование размерностей входных данных и ядра!\n",
    "      # Нужно заполнить Numpy-тензор out \n",
    "\n",
    "      def dot(a, b):\n",
    "          a = np.moveaxis(a, 0, -1)\n",
    "          assert a.shape == b.shape\n",
    "          return (a*b).sum()\n",
    "\n",
    "      input = input_data.copy()\n",
    "      assert self.input_channels == input.shape[1]\n",
    "      k = self.kernel_size\n",
    "      s = self.stride\n",
    "      (batch_count, channels_count) = (input.shape[0], input.shape[1])\n",
    "      (n, m) = (input.shape[2], input.shape[3])\n",
    "      if self.padding == 'same':\n",
    "          pad_n = max(k - (n - 1) % s - 1, 0)\n",
    "          pad_m = max(k - (m - 1) % s - 1, 0)\n",
    "          (up, left) = (pad_n // 2, pad_m // 2)\n",
    "          (down, right) = (pad_n - up, pad_m - left)\n",
    "          input = np.pad(input, pad_width=((0, 0), (0, 0), (up, down),\n",
    "                        (left, right)), mode='constant',\n",
    "                        constant_values=0)\n",
    "      (n, m) = (input.shape[2], input.shape[3])\n",
    "      (out_n, out_m) = ((n - k + 1 + s - 1) // s, (m - k + 1 + s - 1) // s)\n",
    "      assert out_n >= 0\n",
    "      assert out_m >= 0\n",
    "      out = np.zeros([batch_count, self.output_channels, out_n, out_m])\n",
    "      for batch_id in range(batch_count):\n",
    "          for out_ch in range(self.output_channels):\n",
    "              for x in range(0, n - k + 1, s):\n",
    "                  for y in range(0, m - k + 1, s):\n",
    "                      a = input[batch_id, :, x:x + k, y:y + k]\n",
    "                      b = self.kernel[:, :, :, out_ch]\n",
    "                      out[batch_id, out_ch, x // s, y // s] = dot(a, b) \\\n",
    "                          + self.bias[out_ch]\n",
    "      return out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "aUA1PbOBYiH3"
   },
   "outputs": [],
   "source": [
    "def test_Conv2DLayer():\n",
    "  B = 1\n",
    "  C_IN = 1\n",
    "  C_OUT = 1\n",
    "  H = 10\n",
    "  W = 10\n",
    "  K = 3\n",
    "  S = 1\n",
    "  padding = 'same'\n",
    "  x = np.random.randn(B, C_IN, H, W)\n",
    "  K_init = np.random.randn(K, K, C_IN, C_OUT)\n",
    "  b_init = np.random.randn(C_OUT)\n",
    "  y = layers.Conv2D(C_OUT, K, strides=S, padding=padding, data_format='channels_first',\n",
    "    dilation_rate=1, groups=1, activation=None, use_bias=True)\n",
    "  y_keras = y(x).numpy()\n",
    "  y.set_weights([K_init, b_init])\n",
    "  y_keras = y(x).numpy()\n",
    "  y_out = Conv2DLayer(kernel_size=K, input_channels=C_IN, output_channels=C_OUT, \n",
    "                 padding=padding, stride=S, K_init=K_init, b_init=b_init).forward(x)\n",
    "  compare_tensors(y_keras, y_out, tol=0.001, test_name='Test Conv2D 1')\n",
    "  B = 2\n",
    "  C_IN = 3\n",
    "  C_OUT = 5\n",
    "  H = 9\n",
    "  W = 9\n",
    "  K = 3\n",
    "  S = 2\n",
    "  padding = 'valid'\n",
    "  x = np.random.randn(B, C_IN, H, W)\n",
    "  K_init = np.random.randn(K, K, C_IN, C_OUT)\n",
    "  b_init = np.random.randn(C_OUT)\n",
    "  y = layers.Conv2D(C_OUT, K, strides=S, padding=padding, data_format='channels_first',\n",
    "    dilation_rate=1, groups=1, activation=None, use_bias=True, input_shape=(C_IN, H, W))\n",
    "  y_keras = y(x).numpy()\n",
    "  y.set_weights([K_init, b_init])\n",
    "  y_keras = y(x).numpy()\n",
    "  y_out = Conv2DLayer(kernel_size=K, input_channels=C_IN, output_channels=C_OUT, \n",
    "                 padding=padding, stride=S, K_init=K_init, b_init=b_init).forward(x)\n",
    "  compare_tensors(y_keras, y_out, tol=0.001, test_name='Test Conv2D 1')\n",
    "  return"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "cZI5iUu4YiOD",
    "outputId": "f70e287d-29ed-4580-c040-a3b735d58718"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test Conv2D 1 Passed!\n",
      "Test Conv2D 1 Passed!\n"
     ]
    }
   ],
   "source": [
    "test_Conv2DLayer()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "mD0r-HZQMrPb"
   },
   "outputs": [],
   "source": [
    "def my_test_Conv2DLayer():\n",
    "  for iter in range(200):\n",
    "    B = np.random.randint(1,11)\n",
    "    C_IN = np.random.randint(1,6)\n",
    "    C_OUT = np.random.randint(1,6)\n",
    "    H = np.random.randint(1,21)\n",
    "    W = np.random.randint(1,21)\n",
    "    K = np.random.randint(1,6)\n",
    "    S = np.random.randint(1,6)\n",
    "    lst = ['same', 'valid']\n",
    "    padding = lst[np.random.randint(0,2)]\n",
    "    if padding == 'valid' and (H-K+1 < 0 or W-K+1 < 0):\n",
    "        continue\n",
    "    x = np.random.randn(B, C_IN, H, W)\n",
    "    K_init = np.random.randn(K, K, C_IN, C_OUT)\n",
    "    b_init = np.random.randn(C_OUT)\n",
    "    y = layers.Conv2D(C_OUT, K, strides=S, padding=padding, data_format='channels_first',\n",
    "      dilation_rate=1, groups=1, activation=None, use_bias=True)\n",
    "    y_keras = y(x).numpy()\n",
    "    y.set_weights([K_init, b_init])\n",
    "    y_keras = y(x).numpy()\n",
    "    y_out = Conv2DLayer(kernel_size=K, input_channels=C_IN, output_channels=C_OUT, \n",
    "                  padding=padding, stride=S, K_init=K_init, b_init=b_init).forward(x)\n",
    "    compare_tensors(y_keras, y_out, tol=0.001, test_name='Test Conv2D 1')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "3cpN4osP1-wI"
   },
   "outputs": [],
   "source": [
    "my_test_Conv2DLayer()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "91-bDvSvyY2e"
   },
   "source": [
    "\n",
    "\n",
    "---\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "1hPqOr5zad6H"
   },
   "source": [
    "* (2 балла) Реализация **транспонированного сверточного** слоя"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {
    "id": "w7yhHSbexix_"
   },
   "outputs": [],
   "source": [
    "class Conv2DTrLayer(Layer):\n",
    "    def __init__(self, kernel_size=3, input_channels=2, output_channels=3, \n",
    "                 padding=0, stride=1, K_init=None, b_init=None):      \n",
    "      # padding: число (сколько отрезать от модифицированной входной карты)\n",
    "      # Работаем с квадратными ядрами, поэтому kernel_size - одно число\n",
    "      # stride - одно число (коэффициент расширения)\n",
    "      # Фильтр размерности [kernel_size, kernel_size, input_channels, output_channels]\n",
    "      self.name = 'Conv2DTr'\n",
    "      self.kernel_size = kernel_size\n",
    "      self.input_channels = input_channels\n",
    "      self.output_channels = output_channels\n",
    "      self.kernel = K_init\n",
    "      self.bias = b_init\n",
    "      self.padding = padding\n",
    "      self.stride = stride\n",
    "    def forward(self, input_data):\n",
    "      # На входе - четырехмерный тензор вида [batch, input_channels, height, width]\n",
    "      # Вначале нужно проверить на согласование размерностей входных данных и ядра!\n",
    "      # Нужно заполнить Numpy-тензор out \n",
    "\n",
    "      def dot(a, b):\n",
    "          a = np.moveaxis(a, 0, -1)\n",
    "          assert a.shape == b.shape\n",
    "          return (a * b).sum()\n",
    "\n",
    "      input = input_data.copy()\n",
    "      assert self.input_channels == input.shape[1]\n",
    "      k = self.kernel_size\n",
    "      s = self.stride\n",
    "      (batch_count, channels_count) = (input.shape[0], input.shape[1])\n",
    "      (n, m) = (input.shape[2], input.shape[3])\n",
    "      new_n = (n - 1) * (s - 1) + n\n",
    "      new_m = (m - 1) * (s - 1) + m\n",
    "      new_input = np.zeros([batch_count, channels_count, new_n, new_m])\n",
    "      for batch_id in range(batch_count):\n",
    "          for out_ch in range(channels_count):\n",
    "              for x in range(n):\n",
    "                  for y in range(m):\n",
    "                      new_input[batch_id, out_ch, x * s, y * s] = \\\n",
    "                          input[batch_id, out_ch, x, y]\n",
    "      input = new_input\n",
    "      if self.padding == 'same':\n",
    "          pad = s - 1 + k - 1\n",
    "          (pad1, pad2) = (pad // 2, (pad + 1) // 2)\n",
    "          input = np.pad(input, pad_width=((0, 0), (0, 0), (pad1, pad2),\n",
    "                        (pad1, pad2)), mode='constant',\n",
    "                        constant_values=0)\n",
    "      elif self.padding == 'valid':\n",
    "          pad = s - 1 + k - 1 + max(k - s, 0)\n",
    "          (pad1, pad2) = (pad // 2, (pad + 1) // 2)\n",
    "          input = np.pad(input, pad_width=((0, 0), (0, 0), (pad1, pad2),\n",
    "                        (pad1, pad2)), mode='constant',\n",
    "                        constant_values=0)\n",
    "      (n, m) = (input.shape[2], input.shape[3])\n",
    "      (out_n, out_m) = (n - k + 1, m - k + 1)\n",
    "      assert out_n >= 0\n",
    "      assert out_m >= 0\n",
    "      out = np.zeros([batch_count, self.output_channels, out_n, out_m])\n",
    "      for batch_id in range(batch_count):\n",
    "          for out_ch in range(self.output_channels):\n",
    "              for x in range(0, n - k + 1):\n",
    "                  for y in range(0, m - k + 1):\n",
    "                      a = input[batch_id, :, x:x + k, y:y + k]\n",
    "                      b = self.kernel[:, :, :, out_ch]\n",
    "                      out[batch_id, out_ch, x, y] = dot(a, b) \\\n",
    "                          + self.bias[out_ch]\n",
    "      return out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "id": "ueU3lS9-Yi8t"
   },
   "outputs": [],
   "source": [
    "def adjust_kernel(K):\n",
    "  K_new = K.copy()[::-1, ::-1, :, :]\n",
    "  K_new = np.transpose(K_new, (0, 1, 3, 2))\n",
    "  return K_new\n",
    "\n",
    "def test_Conv2DTrLayer():\n",
    "  B = 1\n",
    "  C_IN = 1\n",
    "  C_OUT = 1\n",
    "  H = 3\n",
    "  W = 3\n",
    "  K = 3\n",
    "  S = 2\n",
    "  padding = 0\n",
    "  x = np.random.randn(B, C_IN, H, W)\n",
    "  K_init = np.random.randn(K, K, C_IN, C_OUT)\n",
    "  b_init = np.random.randn(C_OUT)\n",
    "  y = layers.Conv2DTranspose(C_OUT, K, strides=S, padding=\"valid\", output_padding=None, \n",
    "                    data_format='channels_first', dilation_rate=1, groups=1, \n",
    "                    activation=None, use_bias=True)\n",
    "  y_keras = y(x).numpy()\n",
    "  y.set_weights([adjust_kernel(K_init), b_init])\n",
    "  y_keras = y(x).numpy()\n",
    "  y_out = Conv2DTrLayer(kernel_size=K, input_channels=C_IN, output_channels=C_OUT, \n",
    "                 padding=\"valid\", stride=S, K_init=K_init, b_init=b_init).forward(x)\n",
    "  compare_tensors(y_keras, y_out, tol=0.001, test_name='Test Conv2DTr 1')\n",
    "  B = 4\n",
    "  C_IN = 2\n",
    "  C_OUT = 3\n",
    "  H = 3\n",
    "  W = 3\n",
    "  K = 3\n",
    "  S = 2\n",
    "  padding = 0\n",
    "  x = np.random.randn(B, C_IN, H, W)\n",
    "  K_init = np.random.randn(K, K, C_IN, C_OUT)\n",
    "  b_init = np.random.randn(C_OUT)\n",
    "  y = layers.Conv2DTranspose(C_OUT, K, strides=S, padding=\"valid\", output_padding=None, \n",
    "                    data_format='channels_first', dilation_rate=1, groups=1, \n",
    "                    activation=None, use_bias=True)\n",
    "  y_keras = y(x).numpy()\n",
    "  y.set_weights([adjust_kernel(K_init), b_init])\n",
    "  y_keras = y(x).numpy()\n",
    "  y_out = Conv2DTrLayer(kernel_size=K, input_channels=C_IN, output_channels=C_OUT, \n",
    "                 padding=\"valid\", stride=S, K_init=K_init, b_init=b_init).forward(x)\n",
    "  compare_tensors(y_keras, y_out, tol=0.001, test_name='Test Conv2DTr 2')\n",
    "  return"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "rW2REQR3-6dd",
    "outputId": "d3769340-7731-4741-e9ec-9a46220c46ce"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test Conv2DTr 1 Passed!\n",
      "Test Conv2DTr 2 Passed!\n"
     ]
    }
   ],
   "source": [
    "test_Conv2DTrLayer()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {
    "id": "ZCxUympZCJM0"
   },
   "outputs": [],
   "source": [
    "def my_compare_tensors(x, y, tol=0.001):\n",
    "  assert x.shape == y.shape\n",
    "  print(np.amax((x-y)**2))\n",
    "  assert np.amax((x-y)**2) < tol\n",
    "  return"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {
    "id": "kDYn6bI62VF9"
   },
   "outputs": [],
   "source": [
    "def test_new():\n",
    "    for iter in range(300):\n",
    "        B = 1\n",
    "        C_IN = 1\n",
    "        C_OUT = 1\n",
    "        H = np.random.randint(1,10)\n",
    "        W = np.random.randint(1,10)\n",
    "        K = np.random.randint(1,7)\n",
    "        S = np.random.randint(1,7)\n",
    "        if K < S:\n",
    "            K,S = S,K\n",
    "        x = np.random.randn(B, C_IN, H, W)\n",
    "        K_init = np.random.randn(K, K, C_IN, C_OUT)\n",
    "        b_init = np.random.randn(C_OUT)\n",
    "        y = layers.Conv2DTranspose(C_OUT, K, strides=S, padding=\"valid\", output_padding=None, \n",
    "                          data_format='channels_first', dilation_rate=1, groups=1, \n",
    "                          activation=None, use_bias=True)\n",
    "        y_keras = y(x).numpy()\n",
    "        y.set_weights([adjust_kernel(K_init), b_init])\n",
    "        y_keras = y(x).numpy()\n",
    "        y_out = Conv2DTrLayer(kernel_size=K, input_channels=C_IN, output_channels=C_OUT, \n",
    "                      padding=\"valid\", stride=S, K_init=K_init, b_init=b_init).forward(x)\n",
    "        my_compare_tensors(y_keras, y_out, tol=0.00001)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "_f-tscmR2mZI"
   },
   "outputs": [],
   "source": [
    "test_new()"
   ]
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "collapsed_sections": [],
   "name": "practice01_ashirmatov_akmal.ipynb",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
