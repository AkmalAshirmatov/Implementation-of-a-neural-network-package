# Implementation-of-a-neural-network-package

Implementation of main layers in neural layers from scratch:
- Flatten Layer
- Global Average Pooling
- MaxPooling
- Activation layers: ReLU, Sigmoid, Softmax
- BatchNorm
- DenseLayer
- Convolution layer
- Transposed convolution layer

Implementation gradients for every layer

Implementation Network class, able to be constructed from implemented layers.

Implemented back propagation with parameters of a Network from scratch.
